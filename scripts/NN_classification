#!/usr/bin/env python3
import sys
import os
import gc
import shutil
import argparse
import numpy as np
import tensorflow as tf
from math import ceil, floor

from sea_ice_SAR.ML_tools import (
    config_parser,
    calculate_hidden_layer_size,
    process_data,
    learning_curve,
    tr_val_split,
    CNN_stack_shape,
)
from tensorflow.keras import Input
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (
    Dense,
    ZeroPadding3D,
    Conv3D,
    MaxPooling3D,
    Flatten,
    concatenate,
    Dropout,
)
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping

tf.get_logger().setLevel("ERROR")


def NN(
    hidden_layer_size,
    input_layer_size,
    output_layer_size,
    size_2d=None,
    num_layers=None,
    padding_2d=None,
    padding_height=None,
    is_CNN=False,
):
    if is_CNN and num_layers > 4:
        cat_layer_size = input_layer_size + 32
    elif is_CNN and num_layers <= 4:
        cat_layer_size = input_layer_size + 16
    else:
        cat_layer_size = input_layer_size

    hidden_layer_size = calculate_hidden_layer_size(
        cat_layer_size, output_layer_size, hidden_layer_size
    )

    conv_input = Input(shape=(size_2d, size_2d, num_layers, 1), name="conv")
    cat_input = Input(shape=(input_layer_size), name="cat")

    if is_CNN:
        conv_layer = ZeroPadding3D(
            (
                (floor(padding_2d), ceil(padding_2d)),
                (floor(padding_2d), ceil(padding_2d)),
                (floor(padding_height), ceil(padding_height)),
            )
        )(conv_input)
        conv_layer = Conv3D(8, (3, 3, 3), activation="relu", padding="same")(conv_layer)
        conv_layer = MaxPooling3D(pool_size=(2, 2, 2), padding="same")(conv_layer)
        conv_layer = Conv3D(4, (3, 3, 3), activation="relu", padding="same")(conv_layer)
        conv_layer = MaxPooling3D(pool_size=(2, 2, 2), padding="same")(conv_layer)
        flatten_layer = Flatten()(conv_layer)
        # flatten_layer = Dropout(
        #     rate=0.8,
        # )(flatten_layer)
        cat_layer = concatenate([cat_input, flatten_layer])
        hidden_layer = Dense(
            hidden_layer_size, kernel_initializer="normal", activation="relu"
        )(cat_layer)
        # hidden_layer = Dropout(
        #     rate=0.8,
        # )(hidden_layer)
    else:
        hidden_layer = Dense(
            hidden_layer_size, kernel_initializer="normal", activation="relu"
        )(cat_input)
        # hidden_layer = Dropout(
        #     rate=0.8,
        # )(hidden_layer)

    hidden_layer = Dense(
        hidden_layer_size, input_dim=input_layer_size, activation="relu"
    )(hidden_layer)
    # hidden_layer = Dropout(
    #     rate=0.8,
    # )(hidden_layer)
    hidden_layer = Dense(
        hidden_layer_size, input_dim=input_layer_size, activation="relu"
    )(hidden_layer)
    # hidden_layer = Dropout(
    #     rate=0.8,
    # )(hidden_layer)
    hidden_layer = Dense(
        hidden_layer_size, input_dim=input_layer_size, activation="relu"
    )(hidden_layer)
    # hidden_layer = Dropout(
    #     rate=0.8,
    # )(hidden_layer)
    hidden_layer = Dense(
        hidden_layer_size, input_dim=input_layer_size, activation="relu"
    )(hidden_layer)
    # hidden_layer = Dropout(
    #     rate=0.8,
    # )(hidden_layer)
    output_layer = Dense(output_layer_size, activation="softmax")(hidden_layer)

    if is_CNN:
        model = Model(inputs=[conv_input, cat_input], outputs=[output_layer])
    else:
        model = Model(inputs=[cat_input], outputs=[output_layer])

    model.compile(
        loss="categorical_crossentropy", optimizer="adam", metrics=["accuracy"]
    )
    model.summary()

    return model


def main(args):
    # Parse configuration
    (
        train_data,
        _,
        num_epochs,
        hidden_size,
        verbosity,
        K,
        _,
        min_num_points,
        masks,
        CNN_layers,
        _,
        _,
    ) = config_parser(args.ml_config)

    # Set up results directory
    try:
        os.mkdir(args.result_dir)
    except FileExistsError:
        shutil.rmtree(args.result_dir)
        os.mkdir(args.result_dir)

    log_file = open(f"{args.result_dir}/train.log", "w")
    sys.stdout = log_file

    # Modify train dataset
    X_tr, CNN_stacks, Y_tr, _ = process_data(
        train_data,
        args.ml_config,
        regression=False,
        min_num_points=min_num_points,
        masks=masks,
        CNN_layers=CNN_layers,
    )

    size_2d, num_layers, padding_2d, padding_height = CNN_stack_shape(CNN_stacks)
    if type(CNN_stacks) == np.ndarray:
        is_CNN = True
        patience = 100
    else:
        patience = 100
        is_CNN = False

    # Define hidden layer size
    input_layer_size = X_tr.shape[1]
    output_layer_size = len(np.unique(Y_tr))
    if not hidden_size:
        hidden_size = calculate_hidden_layer_size(
            input_layer_size, output_layer_size, hidden_size
        )

    # K-fold classification
    tr_val_pairs = tr_val_split(K, X_tr, Y_tr)

    for iter, (train, validation) in enumerate(tr_val_pairs):
        print(
            f"*************************** Fold #: {iter+1} ***************************",
            file=sys.stdout,
        )
        checkpoint_path = f"{args.result_dir}/ckpt_{iter+1}"
        cp_callback = ModelCheckpoint(
            filepath=checkpoint_path, save_best_only=True, verbose=verbosity, mode="min"
        )
        es_callback = EarlyStopping(
            monitor="val_loss", mode="min", verbose=verbosity, patience=patience
        )

        model = NN(
            hidden_size,
            input_layer_size,
            output_layer_size,
            size_2d=size_2d,
            num_layers=num_layers,
            padding_2d=padding_2d,
            padding_height=padding_height,
            is_CNN=is_CNN,
        )

        # One-hot encoding of the label vector
        one_hot_Y_tr = to_categorical(Y_tr[train])
        one_hot_Y_val = to_categorical(Y_tr[validation])

        if is_CNN:
            x = {"conv": CNN_stacks[train], "cat": X_tr[train]}
            validation_data = (
                {"conv": CNN_stacks[validation], "cat": X_tr[validation]},
                one_hot_Y_val,
            )
        else:
            x = X_tr[train]
            validation_data = (X_tr[validation], one_hot_Y_val)

        # Train the model
        model_summary = model.fit(
            x=x,
            y=one_hot_Y_tr,
            epochs=num_epochs,
            batch_size=1024,
            verbose=verbosity,
            validation_data=validation_data,
            callbacks=[cp_callback, es_callback],
        )

        # Plot the learning curve
        learning_curve(model_summary.history, args.result_dir, iter)

        if K == 1:
            break

    log_file.close()

    # garbage collection
    gc.collect()


if __name__ == "__main__":
    parser = argparse.ArgumentParser()

    parser.add_argument(
        "--result-dir",
        type=str,
        help="Directory path to store the results",
    )
    parser.add_argument(
        "--ml-config",
        nargs="+",
        type=str,
        help="YAML file containing the configuration for deep learning",
    )

    args = parser.parse_args()
    if type(args.ml_config) == list:
        ml_configs = args.ml_config[:]
        for ml_config in ml_configs:
            args.ml_config = ml_config
            main(args)
    else:
        main(args)
