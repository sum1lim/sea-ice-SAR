#!/usr/bin/env python3
import argparse
from math import ceil, floor
import shutil
import sys
import os
import gc
import yaml
import pandas
import numpy as np
import tensorflow as tf

from tensorflow.keras import Input, Model, Sequential, losses
from tensorflow.keras.layers import (
    Conv3D,
    UpSampling3D,
    ZeroPadding3D,
    Cropping3D,
    MaxPooling3D,
    Flatten,
)
from tensorflow.keras.models import load_model
from sea_ice_SAR.ML_tools import process_data, CNN_stack_shape
from sea_ice_SAR.utils import decompose_filepath
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping

tf.get_logger().setLevel("ERROR")


def CAE(input_dimension, num_layers, padding_2d, padding_height):
    autoencoder = Sequential()
    autoencoder.add(Input(shape=(input_dimension, input_dimension, num_layers, 1)))
    autoencoder.add(
        ZeroPadding3D(
            (
                (floor(padding_2d), ceil(padding_2d)),
                (floor(padding_2d), ceil(padding_2d)),
                (floor(padding_height), ceil(padding_height)),
            )
        )
    )
    autoencoder.add(Conv3D(8, (3, 3, 3), activation="relu", padding="same"))
    autoencoder.add(MaxPooling3D(pool_size=(2, 2, 2), padding="same"))
    autoencoder.add(Conv3D(4, (3, 3, 3), activation="relu", padding="same"))
    autoencoder.add(MaxPooling3D(pool_size=(2, 2, 2), padding="same"))

    # end of encoding
    # start of decoding

    autoencoder.add(Conv3D(4, (3, 3, 3), activation="relu", padding="same"))
    autoencoder.add(UpSampling3D((2, 2, 2)))
    autoencoder.add(Conv3D(4, (3, 3, 3), activation="relu", padding="same"))
    autoencoder.add(UpSampling3D((2, 2, 2)))
    autoencoder.add(Conv3D(8, (3, 3, 3), activation="relu", padding="same"))
    autoencoder.add(
        Conv3D(1, kernel_size=(3, 3, 3), activation="sigmoid", padding="same")
    )
    autoencoder.add(
        Cropping3D(
            (
                (floor(padding_2d), ceil(padding_2d)),
                (floor(padding_2d), ceil(padding_2d)),
                (floor(padding_height), ceil(padding_height)),
            )
        )
    )

    autoencoder.compile(optimizer="adam", loss=losses.MeanSquaredError())
    autoencoder.summary()

    return autoencoder


def CAE_output(encoded_df, input_dir, input_file, output_filename, min_num_points=0):
    original_df = pandas.read_csv(f"{input_dir}/{input_file}.csv", header=0)
    original_df.drop(
        original_df[original_df["num_points"] < min_num_points].index, inplace=True
    )
    original_df.reset_index(drop=True, inplace=True)
    original_df.drop(
        original_df.columns.difference(
            ["label", "src_dir", "row", "col", "num_points", "mask"]
        ),
        1,
        inplace=True,
    )
    output_df = pandas.concat([original_df] + encoded_df, axis=1)
    output_df.to_csv(
        f"{input_dir}/{output_filename}.csv",
        index=False,
    )


def convert_to_vectors(df):
    flatten = Sequential()
    flatten.add(Input(shape=df.shape[1:]))
    flatten.add(Flatten())
    flatten.compile()

    return pandas.DataFrame(flatten(df)).astype("float")


def predict(autoencoder, X, dataset_type):
    encoder = Model(autoencoder.layers[0].input, autoencoder.layers[5].output)
    encoder.summary()
    encoded = encoder(X)

    decoded = autoencoder(X)

    print(
        f"Performance on {dataset_type} dataset (mean error): {abs(np.mean(X - decoded))}",
        file=sys.stdout,
    )
    return encoded, decoded


def main(args):
    stream = open(args.config, "r")
    config_dict = yaml.safe_load(stream)

    _, filename, _ = decompose_filepath(args.config)

    try:
        os.mkdir(f"./CAE_models/{filename}")
    except FileExistsError:
        None

    log_file = open(f"./CAE_models/{filename}/train.log", "w")
    sys.stdout = log_file

    for i in range(len(config_dict["train_dir"])):
        checkpoint_path = f"./CAE_models/{filename}/ckpt"

        if not os.path.exists(checkpoint_path):
            _, X_tr, _ = process_data(
                [
                    f"{config_dict['train_dir'][i]}/{f}.csv"
                    for f in config_dict["filenames"]
                ],
                min_num_points=config_dict["min_num_points"],
                CNN_layers=config_dict["filenames"],
                resampling=True,
            )
            _, X_te, _ = process_data(
                [
                    f"{config_dict['test_dir'][i]}/{f}.csv"
                    for f in config_dict["filenames"]
                ],
                min_num_points=config_dict["min_num_points"],
                CNN_layers=config_dict["filenames"],
                resampling=True,
            )

            size_2d, num_layers, padding_2d, padding_height = CNN_stack_shape(X_tr)

            cp_callback = ModelCheckpoint(
                filepath=checkpoint_path,
                save_best_only=True,
                mode="min",
                verbose=config_dict["verbosity"],
            )
            es_callback = EarlyStopping(
                monitor="val_loss",
                mode="min",
                patience=20,
                verbose=config_dict["verbosity"],
            )
            autoencoder = CAE(size_2d, num_layers, padding_2d, padding_height)
            autoencoder.compile(optimizer="adam", loss=losses.MeanSquaredError())

            model_summary = autoencoder.fit(
                X_tr,
                X_tr,
                epochs=config_dict["epochs"],
                batch_size=1024,
                shuffle=True,
                validation_data=(X_te, X_te),
                callbacks=[cp_callback, es_callback],
                verbose=config_dict["verbosity"],
            )

        autoencoder = load_model(checkpoint_path)

        encoded_df_tr = []
        encoded_df_te = []

        _, X_tr, _ = process_data(
            [
                f"{config_dict['train_dir'][i]}/{f}.csv"
                for f in config_dict["filenames"]
            ],
            min_num_points=config_dict["min_num_points"],
            CNN_layers=config_dict["filenames"],
            resampling=False,
        )
        _, X_te, _ = process_data(
            [f"{config_dict['test_dir'][i]}/{f}.csv" for f in config_dict["filenames"]],
            min_num_points=config_dict["min_num_points"],
            CNN_layers=config_dict["filenames"],
            resampling=False,
        )

        encoded_tr, _ = predict(autoencoder, X_tr, "train")
        encoded_te, _ = predict(autoencoder, X_te, "test")

        encoded_df_tr.append(convert_to_vectors(encoded_tr))
        encoded_df_te.append(convert_to_vectors(encoded_te))

        CAE_output(
            encoded_df_tr,
            config_dict["train_dir"][i],
            config_dict["filenames"][0],
            config_dict["output_filename"],
            config_dict["min_num_points"],
        )
        CAE_output(
            encoded_df_te,
            config_dict["test_dir"][i],
            config_dict["filenames"][0],
            config_dict["output_filename"],
            config_dict["min_num_points"],
        )

    log_file.close()

    # garbage collection
    gc.collect()


if __name__ == "__main__":
    parser = argparse.ArgumentParser()

    parser.add_argument(
        "--config",
        nargs="+",
        type=str,
        help="YAML file containing the configuration for CAE",
    )

    args = parser.parse_args()
    args = parser.parse_args()
    if type(args.config) == list:
        configs = args.config[:]
        for config in configs:
            args.config = config
            main(args)
    else:
        main(args)
