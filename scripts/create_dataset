#!/usr/bin/env python3
import argparse
import shutil
import os
import sys
import csv
import yaml
from sea_ice_SAR.data_processing import organize_data, configure_features, SMOTE
from sea_ice_SAR.utils import decompose_filepath


def create_dataset(config_dict, data_split_type, window_size, smote=False):
    feature_cfg = config_dict["features"]

    dataframes = []
    for file_name, file_info in config_dict[data_split_type].items():
        sar_data = file_info["SAR_data"]
        lat_idx = file_info["lat"] - 1
        lon_idx = file_info["lon"] - 1
        label_idx = file_info["label"] - 1

        with open(file_name) as expert_csv:
            csv_reader = csv.reader(expert_csv)
            expert_data = [
                (row[lat_idx], row[lon_idx], row[label_idx])
                for idx, row in enumerate(csv_reader)
                if idx > 0
            ]
            organized_data, feature_li = organize_data(
                expert_data, sar_data, window_size
            )
            dataframes.append(
                configure_features(organized_data, feature_li, feature_cfg, window_size)
            )

    for idx, _ in enumerate(dataframes):
        if idx == 0:
            dataframe_aggr = dataframes[idx]
        else:
            dataframe_aggr = dataframe_aggr.append(dataframes[idx], ignore_index=True)

    if smote:
        dataframe_aggr = SMOTE(dataframe_aggr)

    output_dir, _, _ = decompose_filepath(config_dict["output_dir"][data_split_type])
    try:
        os.mkdir(output_dir)
    except FileExistsError:
        shutil.rmtree(output_dir)
        os.mkdir(output_dir)

    dataframe_aggr.to_csv(config_dict["output_dir"][data_split_type], index=False)
    print(
        f"{data_split_type} saved as {config_dict['output_dir'][data_split_type]}\n",
        file=sys.stdout,
    )


def main(args):
    try:
        stream = open(args.config, "r")
        config_dict = yaml.safe_load(stream)
    except:
        print("ERROR: Configuration file not provided", file=sys.stderr)

    create_dataset(config_dict, "train_data", args.window_size, args.smote)
    create_dataset(config_dict, "test_data", args.window_size, args.smote)


if __name__ == "__main__":
    parser = argparse.ArgumentParser()

    parser.add_argument(
        "--config",
        type=str,
        help="Configuration file to create datasets",
    )
    parser.add_argument(
        "--window-size",
        type=int,
        default=1,
        help="Size of sliding window to collect data from",
    )
    parser.add_argument(
        "--smote",
        action="store_true",
        help="Synthetic Minority Over-Sampling",
    )

    args = parser.parse_args()
    main(args)
