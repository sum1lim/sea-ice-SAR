#!/usr/bin/env python3
import argparse
import os
import sys
import csv
import yaml
from sea_ice_SAR.data_processing import organize_data, configure_features
from sea_ice_SAR.utils import decompose_filepath


def create_dataset(config_dict, data_split_type):
    dataset_cfg = config_dict["configuration"]

    dataframes = []
    for file_name, file_info in config_dict[data_split_type].items():
        sar_data = file_info["SAR_data"]
        lat_idx = file_info["lat"] - 1
        lon_idx = file_info["lon"] - 1
        label_idx = file_info["label"] - 1
        if "Mask" in file_info.keys():
            mask_file = file_info["Mask"]
        else:
            mask_file = None
        try:
            pix_row = file_info["row"] - 1
            pix_col = file_info["col"] - 1
        except KeyError:
            pix_row = None
            pix_col = None

        with open(file_name) as expert_csv:
            csv_reader = csv.reader(expert_csv)
            if pix_row != None and pix_col != None:
                expert_data = [
                    (
                        row[lat_idx],
                        row[lon_idx],
                        row[label_idx],
                        row[pix_row],
                        row[pix_col],
                    )
                    for idx, row in enumerate(csv_reader)
                    if idx > 0
                ]
            else:
                expert_data = [
                    (row[lat_idx], row[lon_idx], row[label_idx])
                    for idx, row in enumerate(csv_reader)
                    if idx > 0
                ]
            organized_data, feature_li = organize_data(
                expert_data,
                sar_data,
                dataset_cfg["window_size"],
                mask_file,
            )
            dataframes.append(configure_features(organized_data, feature_li))

    for idx, _ in enumerate(dataframes):
        if idx == 0:
            dataframe_aggr = dataframes[idx]
        else:
            dataframe_aggr = dataframe_aggr.append(dataframes[idx], ignore_index=True)

    output_dir, _, _ = decompose_filepath(config_dict["output_dir"][data_split_type])
    try:
        os.mkdir(output_dir)
    except FileExistsError:
        None

    dataframe_aggr.to_csv(config_dict["output_dir"][data_split_type], index=False)
    print(
        f"{data_split_type} saved as {config_dict['output_dir'][data_split_type]}\n",
        file=sys.stdout,
    )


def main(args):
    try:
        stream = open(args.config, "r")
        config_dict = yaml.safe_load(stream)
    except:
        print("ERROR: Configuration file not provided", file=sys.stderr)

    create_dataset(config_dict, "train_data")
    if "test_data" in config_dict.keys():
        create_dataset(config_dict, "test_data")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()

    parser.add_argument(
        "--config",
        nargs="+",
        type=str,
        help="Configuration file to create datasets",
    )

    args = parser.parse_args()
    if type(args.config) == list:
        configs = args.config[:]
        for config in configs:
            args.config = config
            main(args)
    else:
        main(args)
