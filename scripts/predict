#!/usr/bin/env python3
import os
import argparse
import tensorflow as tf
import pandas as pd
import numpy as np
from sea_ice_SAR.ML_tools import (
    config_parser,
    process_data,
)
from tensorflow.keras.models import load_model
from sea_ice_SAR.utils import decompose_filepath
from tensorflow.keras.models import load_model

tf.get_logger().setLevel("ERROR")


def main(args):
    # Parse configuration
    train_data, test_data, _, _, _, _, _, _, _, CNN_layers, _, _ = config_parser(
        args.ml_config
    )

    if type(train_data) != list:
        train_data = [train_data]
    if type(test_data) != list:
        test_data = [test_data]

    _, train_filename, _ = decompose_filepath(train_data[0])
    _, test_filename, _ = decompose_filepath(test_data[0])

    # Modify train/test dataset
    X_tr, CNN_stacks_tr, _, _ = process_data(
        train_data,
        args.ml_config,
        regression=False,
        resampling=False,
        dropna=False,
        CNN_layers=CNN_layers,
    )
    X_te, CNN_stacks_te, _, _ = process_data(
        test_data,
        args.ml_config,
        regression=False,
        resampling=False,
        dropna=False,
        CNN_layers=CNN_layers,
    )

    if type(CNN_stacks_tr) == np.ndarray:
        X_tr = {"conv": CNN_stacks_tr, "cat": X_tr}

    if type(CNN_stacks_te) == np.ndarray:
        X_te = {"conv": CNN_stacks_te, "cat": X_te}

    checkpoint_path = f"{args.result_dir}/ckpt_{args.checkpoint}"
    model = load_model(checkpoint_path)
    model.summary()

    predicted_tr = pd.DataFrame(
        model.predict(x=X_tr, batch_size=1000), columns=args.classes
    ).astype(float)
    predicted_te = pd.DataFrame(
        model.predict(x=X_te, batch_size=1000), columns=args.classes
    ).astype(float)

    tr_df = pd.read_csv(train_data[0])
    te_df = pd.read_csv(test_data[0])

    for label in args.classes:
        if label in tr_df.columns:
            tr_df = tr_df.drop(label, axis=1)
            te_df = te_df.drop(label, axis=1)

    output_tr_df = tr_df.join(predicted_tr)
    output_te_df = te_df.join(predicted_te)

    if train_filename.startswith("updated_tr_"):
        os.remove(f"{args.output_dir}/{train_filename}.csv")
        os.remove(f"{args.output_dir}/{test_filename}.csv")
        output_tr_df.to_csv(f"{args.output_dir}/{train_filename}.csv", index=False)
        output_te_df.to_csv(f"{args.output_dir}/{test_filename}.csv", index=False)
    else:
        os.remove(f"{args.output_dir}/updated_tr_{train_filename}.csv")
        os.remove(f"{args.output_dir}/updated_te_{test_filename}.csv")
        output_tr_df.to_csv(
            f"{args.output_dir}/updated_tr_{train_filename}.csv",
            index=False,
        )
        output_te_df.to_csv(
            f"{args.output_dir}/updated_te_{test_filename}.csv",
            index=False,
        )


if __name__ == "__main__":
    parser = argparse.ArgumentParser()

    parser.add_argument(
        "--result-dir",
        type=str,
        help="Directory path with train results including checkpoint files",
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        help="Directory path to output CSV files",
    )
    parser.add_argument(
        "--ml-config",
        nargs="+",
        type=str,
        help="YAML file containing the configuration for deep learning",
    )
    parser.add_argument(
        "--checkpoint",
        type=int,
        help="Checkpoint number of the model to be used",
    )
    parser.add_argument(
        "--classes",
        nargs="+",
        type=str,
        help="Names of the classes",
    )

    args = parser.parse_args()
    if type(args.ml_config) == list:
        ml_configs = args.ml_config[:]
        for ml_config in ml_configs:
            args.ml_config = ml_config
            main(args)
    else:
        main(args)
