[ 13.14519438  30.04553007  46.53366244  63.02179482  79.5099272
  95.99805957 112.48619195 128.97432432 145.4624567  161.95058908
 178.43872145 194.92685383 211.41498621 227.90311858 244.39125096
 260.87938334 277.36751571 293.85564809 310.34378047 326.83191284
 343.32004522 359.80817759 376.29630997 392.78444235 409.27257472
 425.7607071 ]
Before undersampling: [(0, 375), (1, 834), (2, 3661), (3, 4955), (4, 3156), (5, 2425), (6, 2195), (7, 1862), (8, 1648), (9, 1497), (10, 1418), (11, 1313), (12, 1182), (13, 1082), (14, 902), (15, 843), (16, 708), (17, 675), (18, 486), (19, 429), (20, 347), (21, 282), (22, 251), (23, 195), (24, 209)]
After undersampling: [(0, 139), (1, 452), (2, 2528), (3, 3237), (4, 2207), (5, 1690), (6, 1569), (7, 1270), (8, 1184), (9, 973), (10, 1015), (11, 965), (12, 816), (13, 739), (14, 642), (15, 586), (16, 493), (17, 464), (18, 349), (19, 304), (20, 219), (21, 176), (22, 169), (23, 195), (24, 173)]
            label  HH_0_0_x  HV_0_0_x  IA_0_0_x       FYI      DFYI       MYI     rms_0     thk_1     rms_1     rms_2     thk_3     rms_3     thk_4  ...    3         4    5         6         7         8         9        10   11        12   13        14        15                                                CNN
0       28.395086  0.188235  0.066667  0.108728  0.546503  0.361399  0.092098  4.682913  1.174377  4.635949  4.632796  1.186112  4.632457  1.131993  ...  0.0  1.028774  0.0  0.627783  0.600513  0.845440  0.311158  0.945204  0.0  0.748648  0.0  0.725240  0.532229  [[[[0.1764705882352941], [0.1086897756539139],...
1       29.662024  0.184314  0.078431  0.114974  0.823935  0.127742  0.048323  4.474056  1.149307  4.492154  4.467012  1.135941  4.435985  1.091637  ...  0.0  1.037489  0.0  0.626660  0.592701  0.838071  0.262610  0.975056  0.0  0.751528  0.0  0.728721  0.524252  [[[[0.2313725490196078], [0.1149354298909505],...
2       27.557560  0.203922  0.078431  0.152141  0.962041  0.011356  0.026604  4.359618  1.167833  4.431175  4.402533  1.149514  4.336469  1.083636  ...  0.0  0.914490  0.0  0.505533  0.552292  0.701366  0.198895  0.826701  0.0  0.653340  0.0  0.592506  0.477327  [[[[0.1607843137254902], [0.1521730759564568],...
3       25.664618  0.254902  0.058824  0.107278  0.559693  0.362717  0.077590  4.606302  1.164321  4.627554  4.609665  1.157129  4.577537  1.145867  ...  0.0  1.031878  0.0  0.637984  0.593292  0.850416  0.297786  0.964384  0.0  0.749350  0.0  0.734191  0.520697  [[[[0.2431372549019607], [0.1072396671070772],...
4       17.866095  0.270588  0.054902  0.107647  0.291397  0.431503  0.277100  4.943813  1.260254  4.861039  4.867527  1.252281  4.858628  1.218115  ...  0.0  1.002716  0.0  0.648832  0.513782  0.842392  0.308175  0.942157  0.0  0.736924  0.0  0.748369  0.472149  [[[[0.2980392156862745], [0.1076085558124617],...
...           ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...  ...  ...       ...  ...       ...       ...       ...       ...       ...  ...       ...  ...       ...       ...                                                ...
22549  410.150134  0.278431  0.105882  0.080638  0.271956  0.003331  0.724713  4.974597  1.193015  4.883787  4.923381  1.187656  4.904520  1.243723  ...  0.0  1.156851  0.0  0.728820  0.670855  0.950185  0.326065  1.093004  0.0  0.834129  0.0  0.829957  0.573284  [[[[0.3019607843137254], [0.0806800917083141],...
22550  424.042418  0.282353  0.066667  0.149433  0.555378  0.061723  0.382898  4.731061  1.220450  4.833013  4.808225  1.225502  4.756752  1.209594  ...  0.0  0.945098  0.0  0.528045  0.526673  0.715281  0.199913  0.853262  0.0  0.678457  0.0  0.622234  0.460721  [[[[0.2], [0.1494652841605392], [0.06666666666...
22551  421.196328  0.631373  0.466667  0.154089  0.004261  0.000064  0.995674  1.187829  0.197352  0.941815  0.245285  0.157730  0.213412  0.143029  ...  0.0  0.644292  0.0  0.466233  0.184014  0.544459  0.317551  0.496343  0.0  0.488684  0.0  0.521621  0.200857  [[[[0.596078431372549], [0.1541208603802849], ...
22552  416.289186  0.517647  0.160784  0.153404  0.011037  0.000341  0.988622  1.187829  0.197352  0.941815  0.245285  0.157730  0.213412  0.143029  ...  0.0  0.792541  0.0  0.518223  0.461039  0.679398  0.335284  0.699151  0.0  0.512142  0.0  0.532694  0.315921  [[[[0.4078431372549019], [0.1534358903473499],...
22553  423.194662  0.341176  0.274510  0.153307  0.003870  0.000024  0.996105  1.187829  0.197352  0.941815  0.245285  0.157730  0.213412  0.143029  ...  0.0  0.712026  0.0  0.452670  0.356484  0.560431  0.500874  0.445316  0.0  0.508642  0.0  0.462847  0.257479  [[[[0.5686274509803921], [0.1533395804610907],...

[22554 rows x 128 columns]
Size of dataset: (22554, 127)
*************************** Fold #: 1 ***************************
Model: "model"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 conv (InputLayer)              [(None, 7, 7, 3, 1)  0           []                               
                                ]                                                                 
                                                                                                  
 zero_padding3d (ZeroPadding3D)  (None, 8, 8, 4, 1)  0           ['conv[0][0]']                   
                                                                                                  
 conv3d (Conv3D)                (None, 8, 8, 4, 8)   224         ['zero_padding3d[0][0]']         
                                                                                                  
 max_pooling3d (MaxPooling3D)   (None, 4, 4, 2, 8)   0           ['conv3d[0][0]']                 
                                                                                                  
 conv3d_1 (Conv3D)              (None, 4, 4, 2, 4)   868         ['max_pooling3d[0][0]']          
                                                                                                  
 max_pooling3d_1 (MaxPooling3D)  (None, 2, 2, 1, 4)  0           ['conv3d_1[0][0]']               
                                                                                                  
 cat (InputLayer)               [(None, 126)]        0           []                               
                                                                                                  
 flatten (Flatten)              (None, 16)           0           ['max_pooling3d_1[0][0]']        
                                                                                                  
 concatenate (Concatenate)      (None, 142)          0           ['cat[0][0]',                    
                                                                  'flatten[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 200)          28600       ['concatenate[0][0]']            
                                                                                                  
 dense_1 (Dense)                (None, 200)          40200       ['dense[0][0]']                  
                                                                                                  
 dense_2 (Dense)                (None, 200)          40200       ['dense_1[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 200)          40200       ['dense_2[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 200)          40200       ['dense_3[0][0]']                
                                                                                                  
 dense_5 (Dense)                (None, 1)            201         ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 190,693
Trainable params: 190,693
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/10000

Epoch 1: val_loss improved from inf to 2.25491, saving model to ./results/NN_rms_regr/ResNet/recursion_2/ckpt_1
12/12 - 4s - loss: 9.4001 - mse: 9.4001 - mae: 2.6207 - val_loss: 2.2549 - val_mse: 2.2549 - val_mae: 1.2693 - 4s/epoch - 300ms/step
Epoch 2/10000

Epoch 2: val_loss improved from 2.25491 to 0.94169, saving model to ./results/NN_rms_regr/ResNet/recursion_2/ckpt_1
12/12 - 3s - loss: 1.8716 - mse: 1.8716 - mae: 1.0730 - val_loss: 0.9417 - val_mse: 0.9417 - val_mae: 0.5984 - 3s/epoch - 230ms/step
Epoch 3/10000

Epoch 3: val_loss improved from 0.94169 to 0.76526, saving model to ./results/NN_rms_regr/ResNet/recursion_2/ckpt_1
12/12 - 3s - loss: 1.0393 - mse: 1.0393 - mae: 0.6782 - val_loss: 0.7653 - val_mse: 0.7653 - val_mae: 0.5622 - 3s/epoch - 230ms/step
Epoch 4/10000

Epoch 4: val_loss improved from 0.76526 to 0.46168, saving model to ./results/NN_rms_regr/ResNet/recursion_2/ckpt_1
12/12 - 3s - loss: 0.6816 - mse: 0.6816 - mae: 0.5538 - val_loss: 0.4617 - val_mse: 0.4617 - val_mae: 0.5164 - 3s/epoch - 229ms/step
Epoch 5/10000

Epoch 5: val_loss improved from 0.46168 to 0.31409, saving model to ./results/NN_rms_regr/ResNet/recursion_2/ckpt_1
12/12 - 3s - loss: 0.3390 - mse: 0.3390 - mae: 0.4650 - val_loss: 0.3141 - val_mse: 0.3141 - val_mae: 0.4565 - 3s/epoch - 230ms/step
Epoch 6/10000

Epoch 6: val_loss improved from 0.31409 to 0.29380, saving model to ./results/NN_rms_regr/ResNet/recursion_2/ckpt_1
12/12 - 3s - loss: 0.2939 - mse: 0.2939 - mae: 0.4419 - val_loss: 0.2938 - val_mse: 0.2938 - val_mae: 0.4407 - 3s/epoch - 230ms/step
Epoch 7/10000

Epoch 7: val_loss did not improve from 0.29380
12/12 - 2s - loss: 0.2933 - mse: 0.2933 - mae: 0.4421 - val_loss: 0.3064 - val_mse: 0.3064 - val_mae: 0.4482 - 2s/epoch - 167ms/step
Epoch 8/10000

Epoch 8: val_loss improved from 0.29380 to 0.28881, saving model to ./results/NN_rms_regr/ResNet/recursion_2/ckpt_1
12/12 - 3s - loss: 0.2929 - mse: 0.2929 - mae: 0.4395 - val_loss: 0.2888 - val_mse: 0.2888 - val_mae: 0.4359 - 3s/epoch - 244ms/step
Epoch 9/10000

Epoch 9: val_loss did not improve from 0.28881
12/12 - 2s - loss: 0.2912 - mse: 0.2912 - mae: 0.4392 - val_loss: 0.2899 - val_mse: 0.2899 - val_mae: 0.4402 - 2s/epoch - 168ms/step
Epoch 10/10000

Epoch 10: val_loss improved from 0.28881 to 0.28251, saving model to ./results/NN_rms_regr/ResNet/recursion_2/ckpt_1
12/12 - 3s - loss: 0.2838 - mse: 0.2838 - mae: 0.4345 - val_loss: 0.2825 - val_mse: 0.2825 - val_mae: 0.4327 - 3s/epoch - 242ms/step
Epoch 11/10000

Epoch 11: val_loss improved from 0.28251 to 0.28187, saving model to ./results/NN_rms_regr/ResNet/recursion_2/ckpt_1
12/12 - 3s - loss: 0.2825 - mse: 0.2825 - mae: 0.4332 - val_loss: 0.2819 - val_mse: 0.2819 - val_mae: 0.4329 - 3s/epoch - 237ms/step
Epoch 12/10000

Epoch 12: val_loss did not improve from 0.28187
12/12 - 2s - loss: 0.2811 - mse: 0.2811 - mae: 0.4320 - val_loss: 0.2848 - val_mse: 0.2848 - val_mae: 0.4348 - 2s/epoch - 168ms/step
Epoch 13/10000

Epoch 13: val_loss improved from 0.28187 to 0.28099, saving model to ./results/NN_rms_regr/ResNet/recursion_2/ckpt_1
12/12 - 3s - loss: 0.2810 - mse: 0.2810 - mae: 0.4319 - val_loss: 0.2810 - val_mse: 0.2810 - val_mae: 0.4317 - 3s/epoch - 242ms/step
Epoch 14/10000

Epoch 14: val_loss did not improve from 0.28099
12/12 - 2s - loss: 0.2809 - mse: 0.2809 - mae: 0.4316 - val_loss: 0.2843 - val_mse: 0.2843 - val_mae: 0.4330 - 2s/epoch - 168ms/step
Epoch 15/10000

Epoch 15: val_loss did not improve from 0.28099
12/12 - 2s - loss: 0.2818 - mse: 0.2818 - mae: 0.4317 - val_loss: 0.2880 - val_mse: 0.2880 - val_mae: 0.4348 - 2s/epoch - 168ms/step
Epoch 16/10000

Epoch 16: val_loss improved from 0.28099 to 0.28039, saving model to ./results/NN_rms_regr/ResNet/recursion_2/ckpt_1
12/12 - 3s - loss: 0.2863 - mse: 0.2863 - mae: 0.4350 - val_loss: 0.2804 - val_mse: 0.2804 - val_mae: 0.4308 - 3s/epoch - 242ms/step
Epoch 17/10000

Epoch 17: val_loss did not improve from 0.28039
12/12 - 2s - loss: 0.2803 - mse: 0.2803 - mae: 0.4312 - val_loss: 0.2819 - val_mse: 0.2819 - val_mae: 0.4323 - 2s/epoch - 169ms/step
Epoch 18/10000

Epoch 18: val_loss did not improve from 0.28039
12/12 - 2s - loss: 0.2809 - mse: 0.2809 - mae: 0.4315 - val_loss: 0.2868 - val_mse: 0.2868 - val_mae: 0.4355 - 2s/epoch - 167ms/step
Epoch 19/10000

Epoch 19: val_loss did not improve from 0.28039
12/12 - 2s - loss: 0.2845 - mse: 0.2845 - mae: 0.4333 - val_loss: 0.2812 - val_mse: 0.2812 - val_mae: 0.4315 - 2s/epoch - 170ms/step
Epoch 20/10000

Epoch 20: val_loss did not improve from 0.28039
12/12 - 2s - loss: 0.2819 - mse: 0.2819 - mae: 0.4325 - val_loss: 0.3180 - val_mse: 0.3180 - val_mae: 0.4574 - 2s/epoch - 169ms/step
Epoch 21/10000

Epoch 21: val_loss did not improve from 0.28039
12/12 - 2s - loss: 0.2918 - mse: 0.2918 - mae: 0.4381 - val_loss: 0.2807 - val_mse: 0.2807 - val_mae: 0.4317 - 2s/epoch - 168ms/step
Epoch 22/10000

Epoch 22: val_loss did not improve from 0.28039
12/12 - 2s - loss: 0.2808 - mse: 0.2808 - mae: 0.4310 - val_loss: 0.3374 - val_mse: 0.3374 - val_mae: 0.4717 - 2s/epoch - 169ms/step
Epoch 23/10000

Epoch 23: val_loss improved from 0.28039 to 0.27935, saving model to ./results/NN_rms_regr/ResNet/recursion_2/ckpt_1
12/12 - 3s - loss: 0.3008 - mse: 0.3008 - mae: 0.4453 - val_loss: 0.2793 - val_mse: 0.2793 - val_mae: 0.4293 - 3s/epoch - 244ms/step
Epoch 24/10000

Epoch 24: val_loss did not improve from 0.27935
12/12 - 2s - loss: 0.2855 - mse: 0.2855 - mae: 0.4337 - val_loss: 0.2824 - val_mse: 0.2824 - val_mae: 0.4313 - 2s/epoch - 169ms/step
Epoch 25/10000

Epoch 25: val_loss did not improve from 0.27935
12/12 - 2s - loss: 0.2825 - mse: 0.2825 - mae: 0.4323 - val_loss: 0.2974 - val_mse: 0.2974 - val_mae: 0.4392 - 2s/epoch - 169ms/step
Epoch 26/10000

Epoch 26: val_loss did not improve from 0.27935
12/12 - 2s - loss: 0.2834 - mse: 0.2834 - mae: 0.4309 - val_loss: 0.3207 - val_mse: 0.3207 - val_mae: 0.4547 - 2s/epoch - 168ms/step
Epoch 27/10000

Epoch 27: val_loss did not improve from 0.27935
12/12 - 2s - loss: 0.3084 - mse: 0.3084 - mae: 0.4491 - val_loss: 0.4731 - val_mse: 0.4731 - val_mae: 0.5617 - 2s/epoch - 168ms/step
Epoch 28/10000

Epoch 28: val_loss did not improve from 0.27935
12/12 - 2s - loss: 0.3368 - mse: 0.3368 - mae: 0.4676 - val_loss: 0.2881 - val_mse: 0.2881 - val_mae: 0.4348 - 2s/epoch - 168ms/step
Epoch 29/10000

Epoch 29: val_loss did not improve from 0.27935
12/12 - 2s - loss: 0.2856 - mse: 0.2856 - mae: 0.4349 - val_loss: 0.2976 - val_mse: 0.2976 - val_mae: 0.4417 - 2s/epoch - 168ms/step
Epoch 30/10000

Epoch 30: val_loss did not improve from 0.27935
12/12 - 2s - loss: 0.2847 - mse: 0.2847 - mae: 0.4323 - val_loss: 0.3266 - val_mse: 0.3266 - val_mae: 0.4585 - 2s/epoch - 168ms/step
Epoch 31/10000

Epoch 31: val_loss did not improve from 0.27935
12/12 - 2s - loss: 0.2973 - mse: 0.2973 - mae: 0.4416 - val_loss: 0.2808 - val_mse: 0.2808 - val_mae: 0.4300 - 2s/epoch - 169ms/step
Epoch 32/10000

Epoch 32: val_loss did not improve from 0.27935
12/12 - 2s - loss: 0.2847 - mse: 0.2847 - mae: 0.4325 - val_loss: 0.2833 - val_mse: 0.2833 - val_mae: 0.4334 - 2s/epoch - 168ms/step
Epoch 33/10000

Epoch 33: val_loss did not improve from 0.27935
12/12 - 2s - loss: 0.2802 - mse: 0.2802 - mae: 0.4307 - val_loss: 0.3000 - val_mse: 0.3000 - val_mae: 0.4435 - 2s/epoch - 167ms/step
Epoch 34/10000

Epoch 34: val_loss did not improve from 0.27935
12/12 - 2s - loss: 0.2872 - mse: 0.2872 - mae: 0.4340 - val_loss: 0.2921 - val_mse: 0.2921 - val_mae: 0.4363 - 2s/epoch - 168ms/step
Epoch 35/10000

Epoch 35: val_loss did not improve from 0.27935
12/12 - 2s - loss: 0.2871 - mse: 0.2871 - mae: 0.4336 - val_loss: 0.2826 - val_mse: 0.2826 - val_mae: 0.4313 - 2s/epoch - 168ms/step
Epoch 36/10000

Epoch 36: val_loss did not improve from 0.27935
12/12 - 2s - loss: 0.2833 - mse: 0.2833 - mae: 0.4306 - val_loss: 0.2820 - val_mse: 0.2820 - val_mae: 0.4326 - 2s/epoch - 169ms/step
Epoch 37/10000

Epoch 37: val_loss did not improve from 0.27935
12/12 - 2s - loss: 0.2824 - mse: 0.2824 - mae: 0.4322 - val_loss: 0.3486 - val_mse: 0.3486 - val_mae: 0.4775 - 2s/epoch - 167ms/step
Epoch 38/10000

Epoch 38: val_loss did not improve from 0.27935
12/12 - 2s - loss: 0.3085 - mse: 0.3085 - mae: 0.4494 - val_loss: 0.2876 - val_mse: 0.2876 - val_mae: 0.4363 - 2s/epoch - 169ms/step
Epoch 39/10000

Epoch 39: val_loss improved from 0.27935 to 0.27846, saving model to ./results/NN_rms_regr/ResNet/recursion_2/ckpt_1
12/12 - 3s - loss: 0.2950 - mse: 0.2950 - mae: 0.4391 - val_loss: 0.2785 - val_mse: 0.2785 - val_mae: 0.4278 - 3s/epoch - 237ms/step
Epoch 40/10000

Epoch 40: val_loss did not improve from 0.27846
12/12 - 2s - loss: 0.2861 - mse: 0.2861 - mae: 0.4331 - val_loss: 0.2859 - val_mse: 0.2859 - val_mae: 0.4345 - 2s/epoch - 168ms/step
Epoch 41/10000

Epoch 41: val_loss did not improve from 0.27846
12/12 - 2s - loss: 0.2837 - mse: 0.2837 - mae: 0.4323 - val_loss: 0.2786 - val_mse: 0.2786 - val_mae: 0.4277 - 2s/epoch - 168ms/step
Epoch 42/10000

Epoch 42: val_loss did not improve from 0.27846
12/12 - 2s - loss: 0.2781 - mse: 0.2781 - mae: 0.4279 - val_loss: 0.2816 - val_mse: 0.2816 - val_mae: 0.4308 - 2s/epoch - 172ms/step
Epoch 43/10000

Epoch 43: val_loss did not improve from 0.27846
12/12 - 2s - loss: 0.2787 - mse: 0.2787 - mae: 0.4285 - val_loss: 0.3546 - val_mse: 0.3546 - val_mae: 0.4823 - 2s/epoch - 170ms/step
Epoch 44/10000

Epoch 44: val_loss did not improve from 0.27846
12/12 - 2s - loss: 0.3138 - mse: 0.3138 - mae: 0.4522 - val_loss: 0.2800 - val_mse: 0.2800 - val_mae: 0.4294 - 2s/epoch - 169ms/step
Epoch 45/10000

Epoch 45: val_loss improved from 0.27846 to 0.27821, saving model to ./results/NN_rms_regr/ResNet/recursion_2/ckpt_1
12/12 - 3s - loss: 0.2991 - mse: 0.2991 - mae: 0.4409 - val_loss: 0.2782 - val_mse: 0.2782 - val_mae: 0.4283 - 3s/epoch - 243ms/step
Epoch 46/10000

Epoch 46: val_loss did not improve from 0.27821
12/12 - 2s - loss: 0.2995 - mse: 0.2995 - mae: 0.4438 - val_loss: 0.2925 - val_mse: 0.2925 - val_mae: 0.4383 - 2s/epoch - 169ms/step
Epoch 47/10000

Epoch 47: val_loss did not improve from 0.27821
12/12 - 2s - loss: 0.3016 - mse: 0.3016 - mae: 0.4447 - val_loss: 0.2853 - val_mse: 0.2853 - val_mae: 0.4312 - 2s/epoch - 169ms/step
Epoch 48/10000

Epoch 48: val_loss did not improve from 0.27821
12/12 - 2s - loss: 0.2879 - mse: 0.2879 - mae: 0.4353 - val_loss: 0.2787 - val_mse: 0.2787 - val_mae: 0.4281 - 2s/epoch - 169ms/step
Epoch 49/10000

Epoch 49: val_loss did not improve from 0.27821
12/12 - 2s - loss: 0.2794 - mse: 0.2794 - mae: 0.4283 - val_loss: 0.2864 - val_mse: 0.2864 - val_mae: 0.4351 - 2s/epoch - 169ms/step
Epoch 50/10000

Epoch 50: val_loss did not improve from 0.27821
12/12 - 2s - loss: 0.2887 - mse: 0.2887 - mae: 0.4356 - val_loss: 0.2831 - val_mse: 0.2831 - val_mae: 0.4297 - 2s/epoch - 169ms/step
Epoch 51/10000

Epoch 51: val_loss did not improve from 0.27821
12/12 - 2s - loss: 0.2799 - mse: 0.2799 - mae: 0.4290 - val_loss: 0.2856 - val_mse: 0.2856 - val_mae: 0.4327 - 2s/epoch - 169ms/step
Epoch 52/10000

Epoch 52: val_loss did not improve from 0.27821
12/12 - 2s - loss: 0.2810 - mse: 0.2810 - mae: 0.4299 - val_loss: 0.3119 - val_mse: 0.3119 - val_mae: 0.4532 - 2s/epoch - 170ms/step
Epoch 53/10000

Epoch 53: val_loss did not improve from 0.27821
12/12 - 2s - loss: 0.2985 - mse: 0.2985 - mae: 0.4418 - val_loss: 0.2868 - val_mse: 0.2868 - val_mae: 0.4318 - 2s/epoch - 169ms/step
Epoch 54/10000

Epoch 54: val_loss did not improve from 0.27821
12/12 - 2s - loss: 0.2822 - mse: 0.2822 - mae: 0.4309 - val_loss: 0.2861 - val_mse: 0.2861 - val_mae: 0.4321 - 2s/epoch - 170ms/step
Epoch 55/10000

Epoch 55: val_loss did not improve from 0.27821
12/12 - 2s - loss: 0.2792 - mse: 0.2792 - mae: 0.4281 - val_loss: 0.2802 - val_mse: 0.2802 - val_mae: 0.4296 - 2s/epoch - 169ms/step
Epoch 56/10000

Epoch 56: val_loss did not improve from 0.27821
12/12 - 2s - loss: 0.2827 - mse: 0.2827 - mae: 0.4313 - val_loss: 0.2809 - val_mse: 0.2809 - val_mae: 0.4288 - 2s/epoch - 167ms/step
Epoch 57/10000

Epoch 57: val_loss did not improve from 0.27821
12/12 - 2s - loss: 0.2773 - mse: 0.2773 - mae: 0.4269 - val_loss: 0.3126 - val_mse: 0.3126 - val_mae: 0.4487 - 2s/epoch - 170ms/step
Epoch 58/10000

Epoch 58: val_loss did not improve from 0.27821
12/12 - 2s - loss: 0.3052 - mse: 0.3052 - mae: 0.4454 - val_loss: 0.2847 - val_mse: 0.2847 - val_mae: 0.4334 - 2s/epoch - 170ms/step
Epoch 59/10000

Epoch 59: val_loss did not improve from 0.27821
12/12 - 2s - loss: 0.2789 - mse: 0.2789 - mae: 0.4282 - val_loss: 0.2790 - val_mse: 0.2790 - val_mae: 0.4273 - 2s/epoch - 168ms/step
Epoch 60/10000

Epoch 60: val_loss did not improve from 0.27821
12/12 - 2s - loss: 0.2823 - mse: 0.2823 - mae: 0.4305 - val_loss: 0.2954 - val_mse: 0.2954 - val_mae: 0.4403 - 2s/epoch - 169ms/step
Epoch 61/10000

Epoch 61: val_loss did not improve from 0.27821
12/12 - 2s - loss: 0.2838 - mse: 0.2838 - mae: 0.4313 - val_loss: 0.2793 - val_mse: 0.2793 - val_mae: 0.4289 - 2s/epoch - 168ms/step
Epoch 62/10000

Epoch 62: val_loss did not improve from 0.27821
12/12 - 2s - loss: 0.2796 - mse: 0.2796 - mae: 0.4293 - val_loss: 0.2933 - val_mse: 0.2933 - val_mae: 0.4378 - 2s/epoch - 168ms/step
Epoch 63/10000

Epoch 63: val_loss did not improve from 0.27821
12/12 - 2s - loss: 0.2817 - mse: 0.2817 - mae: 0.4294 - val_loss: 0.2808 - val_mse: 0.2808 - val_mae: 0.4304 - 2s/epoch - 167ms/step
Epoch 64/10000

Epoch 64: val_loss did not improve from 0.27821
12/12 - 2s - loss: 0.2771 - mse: 0.2771 - mae: 0.4267 - val_loss: 0.2848 - val_mse: 0.2848 - val_mae: 0.4307 - 2s/epoch - 168ms/step
Epoch 65/10000

Epoch 65: val_loss did not improve from 0.27821
12/12 - 2s - loss: 0.2855 - mse: 0.2855 - mae: 0.4326 - val_loss: 0.3348 - val_mse: 0.3348 - val_mae: 0.4626 - 2s/epoch - 168ms/step
Epoch 66/10000

Epoch 66: val_loss did not improve from 0.27821
12/12 - 2s - loss: 0.2976 - mse: 0.2976 - mae: 0.4399 - val_loss: 0.4291 - val_mse: 0.4291 - val_mae: 0.5235 - 2s/epoch - 169ms/step
Epoch 67/10000

Epoch 67: val_loss did not improve from 0.27821
12/12 - 2s - loss: 0.3296 - mse: 0.3296 - mae: 0.4626 - val_loss: 0.2913 - val_mse: 0.2913 - val_mae: 0.4347 - 2s/epoch - 169ms/step
Epoch 68/10000

Epoch 68: val_loss did not improve from 0.27821
12/12 - 2s - loss: 0.2809 - mse: 0.2809 - mae: 0.4291 - val_loss: 0.2821 - val_mse: 0.2821 - val_mae: 0.4302 - 2s/epoch - 169ms/step
Epoch 69/10000

Epoch 69: val_loss did not improve from 0.27821
12/12 - 2s - loss: 0.2811 - mse: 0.2811 - mae: 0.4297 - val_loss: 0.3205 - val_mse: 0.3205 - val_mae: 0.4552 - 2s/epoch - 169ms/step
Epoch 70/10000

Epoch 70: val_loss did not improve from 0.27821
12/12 - 2s - loss: 0.2925 - mse: 0.2925 - mae: 0.4376 - val_loss: 0.2832 - val_mse: 0.2832 - val_mae: 0.4309 - 2s/epoch - 168ms/step
Epoch 71/10000

Epoch 71: val_loss improved from 0.27821 to 0.27758, saving model to ./results/NN_rms_regr/ResNet/recursion_2/ckpt_1
12/12 - 3s - loss: 0.2796 - mse: 0.2796 - mae: 0.4285 - val_loss: 0.2776 - val_mse: 0.2776 - val_mae: 0.4273 - 3s/epoch - 244ms/step
Epoch 72/10000

Epoch 72: val_loss did not improve from 0.27758
12/12 - 2s - loss: 0.2761 - mse: 0.2761 - mae: 0.4264 - val_loss: 0.3099 - val_mse: 0.3099 - val_mae: 0.4507 - 2s/epoch - 170ms/step
Epoch 73/10000

Epoch 73: val_loss did not improve from 0.27758
12/12 - 2s - loss: 0.2909 - mse: 0.2909 - mae: 0.4358 - val_loss: 0.3020 - val_mse: 0.3020 - val_mae: 0.4455 - 2s/epoch - 168ms/step
Epoch 74/10000

Epoch 74: val_loss improved from 0.27758 to 0.27751, saving model to ./results/NN_rms_regr/ResNet/recursion_2/ckpt_1
12/12 - 3s - loss: 0.2817 - mse: 0.2817 - mae: 0.4302 - val_loss: 0.2775 - val_mse: 0.2775 - val_mae: 0.4269 - 3s/epoch - 238ms/step
Epoch 75/10000

Epoch 75: val_loss did not improve from 0.27751
12/12 - 2s - loss: 0.2766 - mse: 0.2766 - mae: 0.4259 - val_loss: 0.2882 - val_mse: 0.2882 - val_mae: 0.4358 - 2s/epoch - 170ms/step
Epoch 76/10000

Epoch 76: val_loss did not improve from 0.27751
12/12 - 2s - loss: 0.2805 - mse: 0.2805 - mae: 0.4299 - val_loss: 0.3036 - val_mse: 0.3036 - val_mae: 0.4463 - 2s/epoch - 170ms/step
Epoch 77/10000

Epoch 77: val_loss did not improve from 0.27751
12/12 - 2s - loss: 0.2905 - mse: 0.2905 - mae: 0.4368 - val_loss: 0.3225 - val_mse: 0.3225 - val_mae: 0.4599 - 2s/epoch - 169ms/step
Epoch 78/10000

Epoch 78: val_loss did not improve from 0.27751
12/12 - 2s - loss: 0.2892 - mse: 0.2892 - mae: 0.4337 - val_loss: 0.3856 - val_mse: 0.3856 - val_mae: 0.5048 - 2s/epoch - 169ms/step
Epoch 79/10000

Epoch 79: val_loss did not improve from 0.27751
12/12 - 2s - loss: 0.3185 - mse: 0.3185 - mae: 0.4574 - val_loss: 0.3473 - val_mse: 0.3473 - val_mae: 0.4768 - 2s/epoch - 169ms/step
Epoch 80/10000

Epoch 80: val_loss did not improve from 0.27751
12/12 - 2s - loss: 0.3058 - mse: 0.3058 - mae: 0.4470 - val_loss: 0.4300 - val_mse: 0.4300 - val_mae: 0.5332 - 2s/epoch - 168ms/step
Epoch 81/10000

Epoch 81: val_loss did not improve from 0.27751
12/12 - 2s - loss: 0.3312 - mse: 0.3312 - mae: 0.4630 - val_loss: 0.3382 - val_mse: 0.3382 - val_mae: 0.4717 - 2s/epoch - 169ms/step
Epoch 82/10000

Epoch 82: val_loss did not improve from 0.27751
12/12 - 2s - loss: 0.3037 - mse: 0.3037 - mae: 0.4452 - val_loss: 0.3455 - val_mse: 0.3455 - val_mae: 0.4759 - 2s/epoch - 170ms/step
Epoch 83/10000

Epoch 83: val_loss did not improve from 0.27751
12/12 - 2s - loss: 0.2941 - mse: 0.2941 - mae: 0.4386 - val_loss: 0.3677 - val_mse: 0.3677 - val_mae: 0.4909 - 2s/epoch - 170ms/step
Epoch 84/10000

Epoch 84: val_loss did not improve from 0.27751
12/12 - 2s - loss: 0.3157 - mse: 0.3157 - mae: 0.4531 - val_loss: 0.2908 - val_mse: 0.2908 - val_mae: 0.4356 - 2s/epoch - 168ms/step
Epoch 85/10000

Epoch 85: val_loss did not improve from 0.27751
12/12 - 2s - loss: 0.2901 - mse: 0.2901 - mae: 0.4347 - val_loss: 0.3222 - val_mse: 0.3222 - val_mae: 0.4585 - 2s/epoch - 168ms/step
Epoch 86/10000

Epoch 86: val_loss did not improve from 0.27751
12/12 - 2s - loss: 0.2924 - mse: 0.2924 - mae: 0.4373 - val_loss: 0.3216 - val_mse: 0.3216 - val_mae: 0.4600 - 2s/epoch - 169ms/step
Epoch 87/10000

Epoch 87: val_loss did not improve from 0.27751
12/12 - 2s - loss: 0.2966 - mse: 0.2966 - mae: 0.4411 - val_loss: 0.3049 - val_mse: 0.3049 - val_mae: 0.4459 - 2s/epoch - 170ms/step
Epoch 88/10000

Epoch 88: val_loss did not improve from 0.27751
12/12 - 2s - loss: 0.2860 - mse: 0.2860 - mae: 0.4333 - val_loss: 0.2925 - val_mse: 0.2925 - val_mae: 0.4365 - 2s/epoch - 169ms/step
Epoch 89/10000

Epoch 89: val_loss did not improve from 0.27751
12/12 - 2s - loss: 0.2850 - mse: 0.2850 - mae: 0.4316 - val_loss: 0.3024 - val_mse: 0.3024 - val_mae: 0.4423 - 2s/epoch - 168ms/step
Epoch 90/10000

Epoch 90: val_loss did not improve from 0.27751
12/12 - 2s - loss: 0.2911 - mse: 0.2911 - mae: 0.4362 - val_loss: 0.3002 - val_mse: 0.3002 - val_mae: 0.4407 - 2s/epoch - 168ms/step
Epoch 91/10000

Epoch 91: val_loss did not improve from 0.27751
12/12 - 2s - loss: 0.2976 - mse: 0.2976 - mae: 0.4399 - val_loss: 0.3225 - val_mse: 0.3225 - val_mae: 0.4551 - 2s/epoch - 169ms/step
Epoch 92/10000

Epoch 92: val_loss did not improve from 0.27751
12/12 - 2s - loss: 0.2974 - mse: 0.2974 - mae: 0.4410 - val_loss: 0.2816 - val_mse: 0.2816 - val_mae: 0.4286 - 2s/epoch - 169ms/step
Epoch 93/10000

Epoch 93: val_loss did not improve from 0.27751
12/12 - 2s - loss: 0.2772 - mse: 0.2772 - mae: 0.4261 - val_loss: 0.2786 - val_mse: 0.2786 - val_mae: 0.4281 - 2s/epoch - 169ms/step
Epoch 94/10000

Epoch 94: val_loss did not improve from 0.27751
12/12 - 2s - loss: 0.2794 - mse: 0.2794 - mae: 0.4282 - val_loss: 0.2927 - val_mse: 0.2927 - val_mae: 0.4354 - 2s/epoch - 168ms/step
Epoch 95/10000

Epoch 95: val_loss did not improve from 0.27751
12/12 - 2s - loss: 0.2801 - mse: 0.2801 - mae: 0.4290 - val_loss: 0.2781 - val_mse: 0.2781 - val_mae: 0.4270 - 2s/epoch - 170ms/step
Epoch 96/10000

Epoch 96: val_loss did not improve from 0.27751
12/12 - 2s - loss: 0.2766 - mse: 0.2766 - mae: 0.4264 - val_loss: 0.2926 - val_mse: 0.2926 - val_mae: 0.4383 - 2s/epoch - 169ms/step
Epoch 97/10000

Epoch 97: val_loss did not improve from 0.27751
12/12 - 2s - loss: 0.2858 - mse: 0.2858 - mae: 0.4327 - val_loss: 0.3868 - val_mse: 0.3868 - val_mae: 0.5040 - 2s/epoch - 169ms/step
Epoch 98/10000

Epoch 98: val_loss did not improve from 0.27751
12/12 - 2s - loss: 0.3165 - mse: 0.3165 - mae: 0.4533 - val_loss: 0.3005 - val_mse: 0.3005 - val_mae: 0.4416 - 2s/epoch - 169ms/step
Epoch 99/10000

Epoch 99: val_loss did not improve from 0.27751
12/12 - 2s - loss: 0.3076 - mse: 0.3076 - mae: 0.4478 - val_loss: 0.3810 - val_mse: 0.3810 - val_mae: 0.4931 - 2s/epoch - 168ms/step
Epoch 100/10000

Epoch 100: val_loss did not improve from 0.27751
12/12 - 2s - loss: 0.3277 - mse: 0.3277 - mae: 0.4601 - val_loss: 0.2925 - val_mse: 0.2925 - val_mae: 0.4356 - 2s/epoch - 168ms/step
Epoch 101/10000

Epoch 101: val_loss did not improve from 0.27751
12/12 - 2s - loss: 0.2922 - mse: 0.2922 - mae: 0.4361 - val_loss: 0.2776 - val_mse: 0.2776 - val_mae: 0.4272 - 2s/epoch - 169ms/step
Epoch 102/10000

Epoch 102: val_loss did not improve from 0.27751
12/12 - 2s - loss: 0.2763 - mse: 0.2763 - mae: 0.4256 - val_loss: 0.3400 - val_mse: 0.3400 - val_mae: 0.4720 - 2s/epoch - 169ms/step
Epoch 103/10000

Epoch 103: val_loss did not improve from 0.27751
12/12 - 2s - loss: 0.2971 - mse: 0.2971 - mae: 0.4405 - val_loss: 0.2885 - val_mse: 0.2885 - val_mae: 0.4346 - 2s/epoch - 169ms/step
Epoch 104/10000

Epoch 104: val_loss did not improve from 0.27751
12/12 - 2s - loss: 0.2777 - mse: 0.2777 - mae: 0.4264 - val_loss: 0.3193 - val_mse: 0.3193 - val_mae: 0.4541 - 2s/epoch - 174ms/step
Epoch 105/10000

Epoch 105: val_loss did not improve from 0.27751
12/12 - 2s - loss: 0.2946 - mse: 0.2946 - mae: 0.4395 - val_loss: 0.3053 - val_mse: 0.3053 - val_mae: 0.4437 - 2s/epoch - 171ms/step
Epoch 106/10000

Epoch 106: val_loss did not improve from 0.27751
12/12 - 2s - loss: 0.2864 - mse: 0.2864 - mae: 0.4323 - val_loss: 0.3729 - val_mse: 0.3729 - val_mae: 0.4878 - 2s/epoch - 169ms/step
Epoch 107/10000

Epoch 107: val_loss did not improve from 0.27751
12/12 - 2s - loss: 0.3032 - mse: 0.3032 - mae: 0.4442 - val_loss: 0.3545 - val_mse: 0.3545 - val_mae: 0.4767 - 2s/epoch - 168ms/step
Epoch 108/10000

Epoch 108: val_loss did not improve from 0.27751
12/12 - 2s - loss: 0.3239 - mse: 0.3239 - mae: 0.4583 - val_loss: 0.3052 - val_mse: 0.3052 - val_mae: 0.4436 - 2s/epoch - 168ms/step
Epoch 109/10000

Epoch 109: val_loss did not improve from 0.27751
12/12 - 2s - loss: 0.2840 - mse: 0.2840 - mae: 0.4314 - val_loss: 0.3366 - val_mse: 0.3366 - val_mae: 0.4641 - 2s/epoch - 170ms/step
Epoch 110/10000

Epoch 110: val_loss did not improve from 0.27751
12/12 - 2s - loss: 0.3094 - mse: 0.3094 - mae: 0.4483 - val_loss: 0.2839 - val_mse: 0.2839 - val_mae: 0.4332 - 2s/epoch - 169ms/step
Epoch 111/10000

Epoch 111: val_loss did not improve from 0.27751
12/12 - 2s - loss: 0.2785 - mse: 0.2785 - mae: 0.4282 - val_loss: 0.3402 - val_mse: 0.3402 - val_mae: 0.4711 - 2s/epoch - 170ms/step
Epoch 112/10000

Epoch 112: val_loss did not improve from 0.27751
12/12 - 2s - loss: 0.2962 - mse: 0.2962 - mae: 0.4402 - val_loss: 0.3439 - val_mse: 0.3439 - val_mae: 0.4746 - 2s/epoch - 170ms/step
Epoch 113/10000

Epoch 113: val_loss did not improve from 0.27751
12/12 - 2s - loss: 0.2933 - mse: 0.2933 - mae: 0.4386 - val_loss: 0.3038 - val_mse: 0.3038 - val_mae: 0.4462 - 2s/epoch - 169ms/step
Epoch 114/10000

Epoch 114: val_loss did not improve from 0.27751
12/12 - 2s - loss: 0.2890 - mse: 0.2890 - mae: 0.4347 - val_loss: 0.2797 - val_mse: 0.2797 - val_mae: 0.4290 - 2s/epoch - 168ms/step
Epoch 115/10000

Epoch 115: val_loss did not improve from 0.27751
12/12 - 2s - loss: 0.2781 - mse: 0.2781 - mae: 0.4269 - val_loss: 0.3061 - val_mse: 0.3061 - val_mae: 0.4484 - 2s/epoch - 169ms/step
Epoch 116/10000

Epoch 116: val_loss did not improve from 0.27751
12/12 - 2s - loss: 0.2899 - mse: 0.2899 - mae: 0.4364 - val_loss: 0.2787 - val_mse: 0.2787 - val_mae: 0.4273 - 2s/epoch - 168ms/step
Epoch 117/10000

Epoch 117: val_loss did not improve from 0.27751
12/12 - 2s - loss: 0.2771 - mse: 0.2771 - mae: 0.4263 - val_loss: 0.2960 - val_mse: 0.2960 - val_mae: 0.4389 - 2s/epoch - 169ms/step
Epoch 118/10000

Epoch 118: val_loss did not improve from 0.27751
12/12 - 2s - loss: 0.2915 - mse: 0.2915 - mae: 0.4355 - val_loss: 0.2999 - val_mse: 0.2999 - val_mae: 0.4402 - 2s/epoch - 170ms/step
Epoch 119/10000

Epoch 119: val_loss did not improve from 0.27751
12/12 - 2s - loss: 0.2820 - mse: 0.2820 - mae: 0.4297 - val_loss: 0.2780 - val_mse: 0.2780 - val_mae: 0.4278 - 2s/epoch - 168ms/step
Epoch 120/10000

Epoch 120: val_loss did not improve from 0.27751
12/12 - 2s - loss: 0.2764 - mse: 0.2764 - mae: 0.4265 - val_loss: 0.2798 - val_mse: 0.2798 - val_mae: 0.4276 - 2s/epoch - 169ms/step
Epoch 121/10000

Epoch 121: val_loss did not improve from 0.27751
12/12 - 2s - loss: 0.2781 - mse: 0.2781 - mae: 0.4272 - val_loss: 0.2910 - val_mse: 0.2910 - val_mae: 0.4355 - 2s/epoch - 168ms/step
Epoch 122/10000

Epoch 122: val_loss did not improve from 0.27751
12/12 - 2s - loss: 0.2842 - mse: 0.2842 - mae: 0.4309 - val_loss: 0.2859 - val_mse: 0.2859 - val_mae: 0.4337 - 2s/epoch - 169ms/step
Epoch 123/10000

Epoch 123: val_loss did not improve from 0.27751
12/12 - 2s - loss: 0.2841 - mse: 0.2841 - mae: 0.4306 - val_loss: 0.2808 - val_mse: 0.2808 - val_mae: 0.4294 - 2s/epoch - 169ms/step
Epoch 124/10000

Epoch 124: val_loss did not improve from 0.27751
12/12 - 2s - loss: 0.2791 - mse: 0.2791 - mae: 0.4279 - val_loss: 0.2958 - val_mse: 0.2958 - val_mae: 0.4402 - 2s/epoch - 169ms/step
Epoch 124: early stopping
