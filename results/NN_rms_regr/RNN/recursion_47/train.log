[ 13.14088385  30.21795131  46.87850492  63.53905854  80.19961216
  96.86016577 113.52071939 130.181273   146.84182662 163.50238024
 180.16293385 196.82348747 213.48404109 230.1445947  246.80514832
 263.46570194 280.12625555 296.78680917 313.44736279 330.1079164
 346.76847002 363.42902363 380.08957725 396.75013087 413.41068448
 430.0712381 ]
Before undersampling: [(0, 379), (1, 843), (2, 3574), (3, 4722), (4, 3014), (5, 2364), (6, 2085), (7, 1796), (8, 1589), (9, 1446), (10, 1372), (11, 1271), (12, 1163), (13, 1050), (14, 878), (15, 802), (16, 678), (17, 642), (18, 453), (19, 423), (20, 312), (21, 282), (22, 244), (23, 187), (24, 211)]
After undersampling: [(0, 194), (1, 470), (2, 2423), (3, 3125), (4, 2126), (5, 1652), (6, 1499), (7, 1198), (8, 1075), (9, 1002), (10, 942), (11, 888), (12, 822), (13, 794), (14, 660), (15, 540), (16, 490), (17, 460), (18, 308), (19, 284), (20, 203), (21, 179), (22, 175), (23, 187), (24, 171)]
            label  HH_0_0_x  HV_0_0_x  IA_0_0_x       FYI      DFYI       MYI     rms_2     thk_3     rms_3     thk_4     rms_4     thk_0     rms_0  ...         3    4    5         6         7    8    9        10        11   12   13        14        15                                                CNN
0       27.586225  0.227451  0.050980  0.123937  0.870430  0.096906  0.032664  4.374246  1.071508  4.335274  1.061152  4.348178  1.199360  4.380972  ...  1.511440  0.0  0.0  1.481859  0.297430  0.0  0.0  2.537933  1.394098  0.0  0.0  2.271835  1.293874  [[[[0.196078431372549], [0.1239004546520756], ...
1       24.974093  0.188235  0.050980  0.102628  0.785806  0.175785  0.038409  4.592643  1.137596  4.596959  1.134995  4.606903  1.238438  4.624225  ...  0.447709  0.0  0.0  1.393747  0.278140  0.0  0.0  2.083697  0.947357  0.0  0.0  1.931284  0.481822  [[[[0.2], [0.1025880103017769], [0.06666666666...
2       16.899340  0.188235  0.039216  0.089391  0.922564  0.071318  0.006118  4.422366  1.035529  4.436109  1.109292  4.396379  1.106502  4.438368  ...  1.857254  0.0  0.0  2.061486  0.712305  0.0  0.0  1.494994  0.684189  0.0  0.0  1.507465  0.891637  [[[[0.1411764705882353], [0.089433266134823], ...
3       25.472267  0.223529  0.039216  0.103230  0.981389  0.012317  0.006294  4.396954  1.106538  4.336555  1.109434  4.372056  1.180131  4.409079  ...  1.314476  0.0  0.0  1.224548  0.272508  0.0  0.0  2.610754  1.674186  0.0  0.0  1.955072  0.822185  [[[[0.3490196078431372], [0.1031898274141199],...
4       21.526370  0.156863  0.082353  0.152559  0.903778  0.036988  0.059234  4.483117  1.077764  4.439087  1.093017  4.469883  1.235316  4.470763  ...  1.409986  0.0  0.0  1.761705  0.564788  0.0  0.0  1.856859  1.189311  0.0  0.0  1.824010  0.697821  [[[[0.2980392156862745], [0.1525909124636182],...
...           ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...  ...       ...  ...  ...       ...       ...  ...  ...       ...       ...  ...  ...       ...       ...                                                ...
21862  420.320986  0.247059  0.101961  0.092778  0.005510  0.740461  0.254028  5.196586  1.336189  5.169837  1.378204  5.230700  1.354603  5.232620  ...  1.406157  0.0  0.0  1.706077  1.132743  0.0  0.0  2.358022  2.065589  0.0  0.0  1.532413  0.848220  [[[[0.4470588235294118], [0.0928184883267271],...
21863  421.290081  0.211765  0.050980  0.086558  0.633078  0.349396  0.017527  4.550618  1.148247  4.608498  1.109487  4.595167  1.212768  4.621794  ...  0.445693  0.0  0.0  1.612968  0.504233  0.0  0.0  2.320978  0.862513  0.0  0.0  1.403329  0.618954  [[[[0.1725490196078431], [0.0866003522685929],...
21864  424.042418  0.282353  0.066667  0.149433  0.555378  0.061723  0.382898  4.769217  1.096872  4.746229  1.079175  4.754314  1.271293  4.764415  ...  1.048151  0.0  0.0  1.759950  0.344755  0.0  0.0  1.782739  1.367052  0.0  0.0  1.763050  1.095051  [[[[0.2], [0.1494652841605392], [0.06666666666...
21865  417.027580  0.505882  0.090196  0.152162  0.185910  0.031902  0.782188  4.579444  1.247941  4.595232  1.086623  4.599476  1.452081  4.601309  ...  1.035264  0.0  0.0  2.001448  1.379296  0.0  0.0  2.745631  1.093121  0.0  0.0  1.564485  0.810498  [[[[0.4196078431372549], [0.15219440834195], [...
21866  417.866263  0.392157  0.125490  0.152184  0.189279  0.051355  0.759366  4.699316  1.280560  4.704614  1.158046  4.712280  1.448834  4.702883  ...  1.124182  0.0  0.0  2.480716  1.210371  0.0  0.0  2.433277  1.084279  0.0  0.0  1.615430  0.530238  [[[[0.3490196078431372], [0.1522158155254289],...

[21867 rows x 37 columns]
Size of dataset: (21867, 36)
*************************** Fold #: 1 ***************************
Model: "model"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 conv (InputLayer)              [(None, 7, 7, 3, 1)  0           []                               
                                ]                                                                 
                                                                                                  
 zero_padding3d (ZeroPadding3D)  (None, 8, 8, 4, 1)  0           ['conv[0][0]']                   
                                                                                                  
 conv3d (Conv3D)                (None, 8, 8, 4, 8)   224         ['zero_padding3d[0][0]']         
                                                                                                  
 max_pooling3d (MaxPooling3D)   (None, 4, 4, 2, 8)   0           ['conv3d[0][0]']                 
                                                                                                  
 conv3d_1 (Conv3D)              (None, 4, 4, 2, 4)   868         ['max_pooling3d[0][0]']          
                                                                                                  
 max_pooling3d_1 (MaxPooling3D)  (None, 2, 2, 1, 4)  0           ['conv3d_1[0][0]']               
                                                                                                  
 cat (InputLayer)               [(None, 35)]         0           []                               
                                                                                                  
 flatten (Flatten)              (None, 16)           0           ['max_pooling3d_1[0][0]']        
                                                                                                  
 concatenate (Concatenate)      (None, 51)           0           ['cat[0][0]',                    
                                                                  'flatten[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 102)          5304        ['concatenate[0][0]']            
                                                                                                  
 dense_1 (Dense)                (None, 102)          10506       ['dense[0][0]']                  
                                                                                                  
 dense_2 (Dense)                (None, 102)          10506       ['dense_1[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 102)          10506       ['dense_2[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 102)          10506       ['dense_3[0][0]']                
                                                                                                  
 dense_5 (Dense)                (None, 1)            103         ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 48,523
Trainable params: 48,523
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/10000

Epoch 1: val_loss improved from inf to 20.40013, saving model to ./results/NN_rms_regr/RNN/recursion_47/ckpt_1
11/11 - 3s - loss: 23.0097 - mse: 23.0097 - mae: 4.7571 - val_loss: 20.4001 - val_mse: 20.4001 - val_mae: 4.4763 - 3s/epoch - 291ms/step
Epoch 2/10000

Epoch 2: val_loss improved from 20.40013 to 0.37695, saving model to ./results/NN_rms_regr/RNN/recursion_47/ckpt_1
11/11 - 3s - loss: 12.1607 - mse: 12.1607 - mae: 3.2541 - val_loss: 0.3770 - val_mse: 0.3770 - val_mae: 0.4990 - 3s/epoch - 239ms/step
Epoch 3/10000

Epoch 3: val_loss did not improve from 0.37695
11/11 - 2s - loss: 1.8618 - mse: 1.8618 - mae: 1.1283 - val_loss: 1.3715 - val_mse: 1.3715 - val_mae: 1.0428 - 2s/epoch - 173ms/step
Epoch 4/10000

Epoch 4: val_loss did not improve from 0.37695
11/11 - 2s - loss: 1.0910 - mse: 1.0910 - mae: 0.8778 - val_loss: 0.5097 - val_mse: 0.5097 - val_mae: 0.5810 - 2s/epoch - 172ms/step
Epoch 5/10000

Epoch 5: val_loss did not improve from 0.37695
11/11 - 2s - loss: 0.5248 - mse: 0.5248 - mae: 0.5868 - val_loss: 0.4656 - val_mse: 0.4656 - val_mae: 0.5490 - 2s/epoch - 172ms/step
Epoch 6/10000

Epoch 6: val_loss improved from 0.37695 to 0.37036, saving model to ./results/NN_rms_regr/RNN/recursion_47/ckpt_1
11/11 - 3s - loss: 0.4000 - mse: 0.4000 - mae: 0.5100 - val_loss: 0.3704 - val_mse: 0.3704 - val_mae: 0.4947 - 3s/epoch - 238ms/step
Epoch 7/10000

Epoch 7: val_loss improved from 0.37036 to 0.33252, saving model to ./results/NN_rms_regr/RNN/recursion_47/ckpt_1
11/11 - 3s - loss: 0.3446 - mse: 0.3446 - mae: 0.4770 - val_loss: 0.3325 - val_mse: 0.3325 - val_mae: 0.4686 - 3s/epoch - 239ms/step
Epoch 8/10000

Epoch 8: val_loss improved from 0.33252 to 0.31370, saving model to ./results/NN_rms_regr/RNN/recursion_47/ckpt_1
11/11 - 3s - loss: 0.3246 - mse: 0.3246 - mae: 0.4654 - val_loss: 0.3137 - val_mse: 0.3137 - val_mae: 0.4578 - 3s/epoch - 239ms/step
Epoch 9/10000

Epoch 9: val_loss improved from 0.31370 to 0.31049, saving model to ./results/NN_rms_regr/RNN/recursion_47/ckpt_1
11/11 - 3s - loss: 0.3201 - mse: 0.3201 - mae: 0.4622 - val_loss: 0.3105 - val_mse: 0.3105 - val_mae: 0.4554 - 3s/epoch - 239ms/step
Epoch 10/10000

Epoch 10: val_loss improved from 0.31049 to 0.30996, saving model to ./results/NN_rms_regr/RNN/recursion_47/ckpt_1
11/11 - 3s - loss: 0.3178 - mse: 0.3178 - mae: 0.4608 - val_loss: 0.3100 - val_mse: 0.3100 - val_mae: 0.4548 - 3s/epoch - 254ms/step
Epoch 11/10000

Epoch 11: val_loss improved from 0.30996 to 0.30909, saving model to ./results/NN_rms_regr/RNN/recursion_47/ckpt_1
11/11 - 3s - loss: 0.3168 - mse: 0.3168 - mae: 0.4599 - val_loss: 0.3091 - val_mse: 0.3091 - val_mae: 0.4542 - 3s/epoch - 253ms/step
Epoch 12/10000

Epoch 12: val_loss improved from 0.30909 to 0.30855, saving model to ./results/NN_rms_regr/RNN/recursion_47/ckpt_1
11/11 - 3s - loss: 0.3160 - mse: 0.3160 - mae: 0.4594 - val_loss: 0.3085 - val_mse: 0.3085 - val_mae: 0.4536 - 3s/epoch - 254ms/step
Epoch 13/10000

Epoch 13: val_loss improved from 0.30855 to 0.30769, saving model to ./results/NN_rms_regr/RNN/recursion_47/ckpt_1
11/11 - 3s - loss: 0.3154 - mse: 0.3154 - mae: 0.4587 - val_loss: 0.3077 - val_mse: 0.3077 - val_mae: 0.4531 - 3s/epoch - 248ms/step
Epoch 14/10000

Epoch 14: val_loss improved from 0.30769 to 0.30702, saving model to ./results/NN_rms_regr/RNN/recursion_47/ckpt_1
11/11 - 3s - loss: 0.3147 - mse: 0.3147 - mae: 0.4582 - val_loss: 0.3070 - val_mse: 0.3070 - val_mae: 0.4524 - 3s/epoch - 253ms/step
Epoch 15/10000

Epoch 15: val_loss improved from 0.30702 to 0.30633, saving model to ./results/NN_rms_regr/RNN/recursion_47/ckpt_1
11/11 - 3s - loss: 0.3140 - mse: 0.3140 - mae: 0.4577 - val_loss: 0.3063 - val_mse: 0.3063 - val_mae: 0.4518 - 3s/epoch - 247ms/step
Epoch 16/10000

Epoch 16: val_loss improved from 0.30633 to 0.30550, saving model to ./results/NN_rms_regr/RNN/recursion_47/ckpt_1
11/11 - 3s - loss: 0.3132 - mse: 0.3132 - mae: 0.4571 - val_loss: 0.3055 - val_mse: 0.3055 - val_mae: 0.4512 - 3s/epoch - 253ms/step
Epoch 17/10000

Epoch 17: val_loss improved from 0.30550 to 0.30507, saving model to ./results/NN_rms_regr/RNN/recursion_47/ckpt_1
11/11 - 3s - loss: 0.3124 - mse: 0.3124 - mae: 0.4563 - val_loss: 0.3051 - val_mse: 0.3051 - val_mae: 0.4506 - 3s/epoch - 253ms/step
Epoch 18/10000

Epoch 18: val_loss improved from 0.30507 to 0.30422, saving model to ./results/NN_rms_regr/RNN/recursion_47/ckpt_1
11/11 - 3s - loss: 0.3117 - mse: 0.3117 - mae: 0.4558 - val_loss: 0.3042 - val_mse: 0.3042 - val_mae: 0.4500 - 3s/epoch - 253ms/step
Epoch 19/10000

Epoch 19: val_loss improved from 0.30422 to 0.30339, saving model to ./results/NN_rms_regr/RNN/recursion_47/ckpt_1
11/11 - 3s - loss: 0.3111 - mse: 0.3111 - mae: 0.4552 - val_loss: 0.3034 - val_mse: 0.3034 - val_mae: 0.4494 - 3s/epoch - 248ms/step
Epoch 20/10000

Epoch 20: val_loss improved from 0.30339 to 0.30275, saving model to ./results/NN_rms_regr/RNN/recursion_47/ckpt_1
11/11 - 3s - loss: 0.3103 - mse: 0.3103 - mae: 0.4545 - val_loss: 0.3028 - val_mse: 0.3028 - val_mae: 0.4487 - 3s/epoch - 253ms/step
Epoch 21/10000

Epoch 21: val_loss improved from 0.30275 to 0.30237, saving model to ./results/NN_rms_regr/RNN/recursion_47/ckpt_1
11/11 - 3s - loss: 0.3095 - mse: 0.3095 - mae: 0.4538 - val_loss: 0.3024 - val_mse: 0.3024 - val_mae: 0.4484 - 3s/epoch - 254ms/step
Epoch 22/10000

Epoch 22: val_loss improved from 0.30237 to 0.30170, saving model to ./results/NN_rms_regr/RNN/recursion_47/ckpt_1
11/11 - 3s - loss: 0.3093 - mse: 0.3093 - mae: 0.4536 - val_loss: 0.3017 - val_mse: 0.3017 - val_mae: 0.4477 - 3s/epoch - 247ms/step
Epoch 23/10000

Epoch 23: val_loss improved from 0.30170 to 0.30076, saving model to ./results/NN_rms_regr/RNN/recursion_47/ckpt_1
11/11 - 3s - loss: 0.3081 - mse: 0.3081 - mae: 0.4527 - val_loss: 0.3008 - val_mse: 0.3008 - val_mae: 0.4470 - 3s/epoch - 254ms/step
Epoch 24/10000

Epoch 24: val_loss improved from 0.30076 to 0.30033, saving model to ./results/NN_rms_regr/RNN/recursion_47/ckpt_1
11/11 - 3s - loss: 0.3079 - mse: 0.3079 - mae: 0.4525 - val_loss: 0.3003 - val_mse: 0.3003 - val_mae: 0.4465 - 3s/epoch - 253ms/step
Epoch 25/10000

Epoch 25: val_loss improved from 0.30033 to 0.30030, saving model to ./results/NN_rms_regr/RNN/recursion_47/ckpt_1
11/11 - 3s - loss: 0.3070 - mse: 0.3070 - mae: 0.4516 - val_loss: 0.3003 - val_mse: 0.3003 - val_mae: 0.4464 - 3s/epoch - 249ms/step
Epoch 26/10000

Epoch 26: val_loss improved from 0.30030 to 0.29959, saving model to ./results/NN_rms_regr/RNN/recursion_47/ckpt_1
11/11 - 3s - loss: 0.3069 - mse: 0.3069 - mae: 0.4515 - val_loss: 0.2996 - val_mse: 0.2996 - val_mae: 0.4458 - 3s/epoch - 252ms/step
Epoch 27/10000

Epoch 27: val_loss improved from 0.29959 to 0.29847, saving model to ./results/NN_rms_regr/RNN/recursion_47/ckpt_1
11/11 - 3s - loss: 0.3069 - mse: 0.3069 - mae: 0.4516 - val_loss: 0.2985 - val_mse: 0.2985 - val_mae: 0.4449 - 3s/epoch - 253ms/step
Epoch 28/10000

Epoch 28: val_loss did not improve from 0.29847
11/11 - 2s - loss: 0.3056 - mse: 0.3056 - mae: 0.4505 - val_loss: 0.2985 - val_mse: 0.2985 - val_mae: 0.4448 - 2s/epoch - 172ms/step
Epoch 29/10000

Epoch 29: val_loss improved from 0.29847 to 0.29744, saving model to ./results/NN_rms_regr/RNN/recursion_47/ckpt_1
11/11 - 3s - loss: 0.3056 - mse: 0.3056 - mae: 0.4507 - val_loss: 0.2974 - val_mse: 0.2974 - val_mae: 0.4440 - 3s/epoch - 253ms/step
Epoch 30/10000

Epoch 30: val_loss improved from 0.29744 to 0.29706, saving model to ./results/NN_rms_regr/RNN/recursion_47/ckpt_1
11/11 - 3s - loss: 0.3040 - mse: 0.3040 - mae: 0.4492 - val_loss: 0.2971 - val_mse: 0.2971 - val_mae: 0.4436 - 3s/epoch - 247ms/step
Epoch 31/10000

Epoch 31: val_loss improved from 0.29706 to 0.29688, saving model to ./results/NN_rms_regr/RNN/recursion_47/ckpt_1
11/11 - 3s - loss: 0.3036 - mse: 0.3036 - mae: 0.4487 - val_loss: 0.2969 - val_mse: 0.2969 - val_mae: 0.4433 - 3s/epoch - 252ms/step
Epoch 32/10000

Epoch 32: val_loss improved from 0.29688 to 0.29671, saving model to ./results/NN_rms_regr/RNN/recursion_47/ckpt_1
11/11 - 3s - loss: 0.3033 - mse: 0.3033 - mae: 0.4483 - val_loss: 0.2967 - val_mse: 0.2967 - val_mae: 0.4431 - 3s/epoch - 252ms/step
Epoch 33/10000

Epoch 33: val_loss improved from 0.29671 to 0.29528, saving model to ./results/NN_rms_regr/RNN/recursion_47/ckpt_1
11/11 - 3s - loss: 0.3023 - mse: 0.3023 - mae: 0.4476 - val_loss: 0.2953 - val_mse: 0.2953 - val_mae: 0.4420 - 3s/epoch - 253ms/step
Epoch 34/10000

Epoch 34: val_loss improved from 0.29528 to 0.29474, saving model to ./results/NN_rms_regr/RNN/recursion_47/ckpt_1
11/11 - 3s - loss: 0.3019 - mse: 0.3019 - mae: 0.4472 - val_loss: 0.2947 - val_mse: 0.2947 - val_mae: 0.4415 - 3s/epoch - 246ms/step
Epoch 35/10000

Epoch 35: val_loss did not improve from 0.29474
11/11 - 2s - loss: 0.3018 - mse: 0.3018 - mae: 0.4471 - val_loss: 0.2948 - val_mse: 0.2948 - val_mae: 0.4414 - 2s/epoch - 172ms/step
Epoch 36/10000

Epoch 36: val_loss improved from 0.29474 to 0.29406, saving model to ./results/NN_rms_regr/RNN/recursion_47/ckpt_1
11/11 - 3s - loss: 0.3015 - mse: 0.3015 - mae: 0.4468 - val_loss: 0.2941 - val_mse: 0.2941 - val_mae: 0.4408 - 3s/epoch - 253ms/step
Epoch 37/10000

Epoch 37: val_loss improved from 0.29406 to 0.29364, saving model to ./results/NN_rms_regr/RNN/recursion_47/ckpt_1
11/11 - 3s - loss: 0.3004 - mse: 0.3004 - mae: 0.4458 - val_loss: 0.2936 - val_mse: 0.2936 - val_mae: 0.4405 - 3s/epoch - 254ms/step
Epoch 38/10000

Epoch 38: val_loss improved from 0.29364 to 0.29310, saving model to ./results/NN_rms_regr/RNN/recursion_47/ckpt_1
11/11 - 3s - loss: 0.3004 - mse: 0.3004 - mae: 0.4458 - val_loss: 0.2931 - val_mse: 0.2931 - val_mae: 0.4400 - 3s/epoch - 247ms/step
Epoch 39/10000

Epoch 39: val_loss improved from 0.29310 to 0.29298, saving model to ./results/NN_rms_regr/RNN/recursion_47/ckpt_1
11/11 - 3s - loss: 0.2997 - mse: 0.2997 - mae: 0.4452 - val_loss: 0.2930 - val_mse: 0.2930 - val_mae: 0.4398 - 3s/epoch - 253ms/step
Epoch 40/10000

Epoch 40: val_loss improved from 0.29298 to 0.29238, saving model to ./results/NN_rms_regr/RNN/recursion_47/ckpt_1
11/11 - 3s - loss: 0.2994 - mse: 0.2994 - mae: 0.4450 - val_loss: 0.2924 - val_mse: 0.2924 - val_mae: 0.4393 - 3s/epoch - 255ms/step
Epoch 41/10000

Epoch 41: val_loss improved from 0.29238 to 0.29223, saving model to ./results/NN_rms_regr/RNN/recursion_47/ckpt_1
11/11 - 3s - loss: 0.2990 - mse: 0.2990 - mae: 0.4445 - val_loss: 0.2922 - val_mse: 0.2922 - val_mae: 0.4391 - 3s/epoch - 250ms/step
Epoch 42/10000

Epoch 42: val_loss improved from 0.29223 to 0.29178, saving model to ./results/NN_rms_regr/RNN/recursion_47/ckpt_1
11/11 - 3s - loss: 0.2987 - mse: 0.2987 - mae: 0.4442 - val_loss: 0.2918 - val_mse: 0.2918 - val_mae: 0.4387 - 3s/epoch - 255ms/step
Epoch 43/10000

Epoch 43: val_loss improved from 0.29178 to 0.29151, saving model to ./results/NN_rms_regr/RNN/recursion_47/ckpt_1
11/11 - 3s - loss: 0.2982 - mse: 0.2982 - mae: 0.4438 - val_loss: 0.2915 - val_mse: 0.2915 - val_mae: 0.4384 - 3s/epoch - 256ms/step
Epoch 44/10000

Epoch 44: val_loss improved from 0.29151 to 0.29120, saving model to ./results/NN_rms_regr/RNN/recursion_47/ckpt_1
11/11 - 3s - loss: 0.2982 - mse: 0.2982 - mae: 0.4437 - val_loss: 0.2912 - val_mse: 0.2912 - val_mae: 0.4381 - 3s/epoch - 252ms/step
Epoch 45/10000

Epoch 45: val_loss improved from 0.29120 to 0.29095, saving model to ./results/NN_rms_regr/RNN/recursion_47/ckpt_1
11/11 - 3s - loss: 0.2982 - mse: 0.2982 - mae: 0.4435 - val_loss: 0.2909 - val_mse: 0.2909 - val_mae: 0.4379 - 3s/epoch - 259ms/step
Epoch 46/10000

Epoch 46: val_loss improved from 0.29095 to 0.29088, saving model to ./results/NN_rms_regr/RNN/recursion_47/ckpt_1
11/11 - 3s - loss: 0.2973 - mse: 0.2973 - mae: 0.4430 - val_loss: 0.2909 - val_mse: 0.2909 - val_mae: 0.4378 - 3s/epoch - 248ms/step
Epoch 47/10000

Epoch 47: val_loss improved from 0.29088 to 0.29055, saving model to ./results/NN_rms_regr/RNN/recursion_47/ckpt_1
11/11 - 3s - loss: 0.2978 - mse: 0.2978 - mae: 0.4432 - val_loss: 0.2905 - val_mse: 0.2905 - val_mae: 0.4374 - 3s/epoch - 260ms/step
Epoch 48/10000

Epoch 48: val_loss improved from 0.29055 to 0.29026, saving model to ./results/NN_rms_regr/RNN/recursion_47/ckpt_1
11/11 - 3s - loss: 0.2976 - mse: 0.2976 - mae: 0.4429 - val_loss: 0.2903 - val_mse: 0.2903 - val_mae: 0.4372 - 3s/epoch - 259ms/step
Epoch 49/10000

Epoch 49: val_loss did not improve from 0.29026
11/11 - 2s - loss: 0.2968 - mse: 0.2968 - mae: 0.4426 - val_loss: 0.2908 - val_mse: 0.2908 - val_mae: 0.4374 - 2s/epoch - 178ms/step
Epoch 50/10000

Epoch 50: val_loss improved from 0.29026 to 0.28995, saving model to ./results/NN_rms_regr/RNN/recursion_47/ckpt_1
11/11 - 3s - loss: 0.2965 - mse: 0.2965 - mae: 0.4420 - val_loss: 0.2900 - val_mse: 0.2900 - val_mae: 0.4368 - 3s/epoch - 259ms/step
Epoch 51/10000

Epoch 51: val_loss did not improve from 0.28995
11/11 - 2s - loss: 0.2961 - mse: 0.2961 - mae: 0.4418 - val_loss: 0.2902 - val_mse: 0.2902 - val_mae: 0.4369 - 2s/epoch - 176ms/step
Epoch 52/10000

Epoch 52: val_loss did not improve from 0.28995
11/11 - 2s - loss: 0.2966 - mse: 0.2966 - mae: 0.4422 - val_loss: 0.2907 - val_mse: 0.2907 - val_mae: 0.4371 - 2s/epoch - 172ms/step
Epoch 53/10000

Epoch 53: val_loss did not improve from 0.28995
11/11 - 2s - loss: 0.2967 - mse: 0.2967 - mae: 0.4418 - val_loss: 0.2910 - val_mse: 0.2910 - val_mae: 0.4373 - 2s/epoch - 179ms/step
Epoch 54/10000

Epoch 54: val_loss improved from 0.28995 to 0.28924, saving model to ./results/NN_rms_regr/RNN/recursion_47/ckpt_1
11/11 - 3s - loss: 0.2977 - mse: 0.2977 - mae: 0.4425 - val_loss: 0.2892 - val_mse: 0.2892 - val_mae: 0.4361 - 3s/epoch - 248ms/step
Epoch 55/10000

Epoch 55: val_loss did not improve from 0.28924
11/11 - 2s - loss: 0.2964 - mse: 0.2964 - mae: 0.4418 - val_loss: 0.2908 - val_mse: 0.2908 - val_mae: 0.4371 - 2s/epoch - 173ms/step
Epoch 56/10000

Epoch 56: val_loss improved from 0.28924 to 0.28874, saving model to ./results/NN_rms_regr/RNN/recursion_47/ckpt_1
11/11 - 3s - loss: 0.2954 - mse: 0.2954 - mae: 0.4411 - val_loss: 0.2887 - val_mse: 0.2887 - val_mae: 0.4356 - 3s/epoch - 257ms/step
Epoch 57/10000

Epoch 57: val_loss improved from 0.28874 to 0.28872, saving model to ./results/NN_rms_regr/RNN/recursion_47/ckpt_1
11/11 - 3s - loss: 0.2949 - mse: 0.2949 - mae: 0.4405 - val_loss: 0.2887 - val_mse: 0.2887 - val_mae: 0.4356 - 3s/epoch - 260ms/step
Epoch 58/10000

Epoch 58: val_loss did not improve from 0.28872
11/11 - 2s - loss: 0.2952 - mse: 0.2952 - mae: 0.4408 - val_loss: 0.2906 - val_mse: 0.2906 - val_mae: 0.4367 - 2s/epoch - 174ms/step
Epoch 59/10000

Epoch 59: val_loss did not improve from 0.28872
11/11 - 2s - loss: 0.2953 - mse: 0.2953 - mae: 0.4409 - val_loss: 0.2894 - val_mse: 0.2894 - val_mae: 0.4358 - 2s/epoch - 174ms/step
Epoch 60/10000

Epoch 60: val_loss improved from 0.28872 to 0.28826, saving model to ./results/NN_rms_regr/RNN/recursion_47/ckpt_1
11/11 - 3s - loss: 0.2949 - mse: 0.2949 - mae: 0.4405 - val_loss: 0.2883 - val_mse: 0.2883 - val_mae: 0.4351 - 3s/epoch - 247ms/step
Epoch 61/10000

Epoch 61: val_loss did not improve from 0.28826
11/11 - 2s - loss: 0.2943 - mse: 0.2943 - mae: 0.4400 - val_loss: 0.2884 - val_mse: 0.2884 - val_mae: 0.4351 - 2s/epoch - 173ms/step
Epoch 62/10000

Epoch 62: val_loss did not improve from 0.28826
11/11 - 2s - loss: 0.2944 - mse: 0.2944 - mae: 0.4402 - val_loss: 0.2883 - val_mse: 0.2883 - val_mae: 0.4350 - 2s/epoch - 173ms/step
Epoch 63/10000

Epoch 63: val_loss improved from 0.28826 to 0.28812, saving model to ./results/NN_rms_regr/RNN/recursion_47/ckpt_1
11/11 - 3s - loss: 0.2941 - mse: 0.2941 - mae: 0.4399 - val_loss: 0.2881 - val_mse: 0.2881 - val_mae: 0.4348 - 3s/epoch - 254ms/step
Epoch 64/10000

Epoch 64: val_loss improved from 0.28812 to 0.28790, saving model to ./results/NN_rms_regr/RNN/recursion_47/ckpt_1
11/11 - 3s - loss: 0.2942 - mse: 0.2942 - mae: 0.4398 - val_loss: 0.2879 - val_mse: 0.2879 - val_mae: 0.4346 - 3s/epoch - 248ms/step
Epoch 65/10000

Epoch 65: val_loss did not improve from 0.28790
11/11 - 2s - loss: 0.2943 - mse: 0.2943 - mae: 0.4396 - val_loss: 0.2880 - val_mse: 0.2880 - val_mae: 0.4347 - 2s/epoch - 173ms/step
Epoch 66/10000

Epoch 66: val_loss did not improve from 0.28790
11/11 - 2s - loss: 0.2938 - mse: 0.2938 - mae: 0.4394 - val_loss: 0.2880 - val_mse: 0.2880 - val_mae: 0.4346 - 2s/epoch - 174ms/step
Epoch 67/10000

Epoch 67: val_loss improved from 0.28790 to 0.28779, saving model to ./results/NN_rms_regr/RNN/recursion_47/ckpt_1
11/11 - 3s - loss: 0.2937 - mse: 0.2937 - mae: 0.4394 - val_loss: 0.2878 - val_mse: 0.2878 - val_mae: 0.4344 - 3s/epoch - 253ms/step
Epoch 68/10000

Epoch 68: val_loss improved from 0.28779 to 0.28771, saving model to ./results/NN_rms_regr/RNN/recursion_47/ckpt_1
11/11 - 3s - loss: 0.2936 - mse: 0.2936 - mae: 0.4392 - val_loss: 0.2877 - val_mse: 0.2877 - val_mae: 0.4343 - 3s/epoch - 255ms/step
Epoch 69/10000

Epoch 69: val_loss did not improve from 0.28771
11/11 - 2s - loss: 0.2936 - mse: 0.2936 - mae: 0.4393 - val_loss: 0.2878 - val_mse: 0.2878 - val_mae: 0.4344 - 2s/epoch - 174ms/step
Epoch 70/10000

Epoch 70: val_loss improved from 0.28771 to 0.28756, saving model to ./results/NN_rms_regr/RNN/recursion_47/ckpt_1
11/11 - 3s - loss: 0.2936 - mse: 0.2936 - mae: 0.4391 - val_loss: 0.2876 - val_mse: 0.2876 - val_mae: 0.4341 - 3s/epoch - 248ms/step
Epoch 71/10000

Epoch 71: val_loss did not improve from 0.28756
11/11 - 2s - loss: 0.2935 - mse: 0.2935 - mae: 0.4391 - val_loss: 0.2876 - val_mse: 0.2876 - val_mae: 0.4341 - 2s/epoch - 174ms/step
Epoch 72/10000

Epoch 72: val_loss did not improve from 0.28756
11/11 - 2s - loss: 0.2933 - mse: 0.2933 - mae: 0.4389 - val_loss: 0.2877 - val_mse: 0.2877 - val_mae: 0.4341 - 2s/epoch - 174ms/step
Epoch 73/10000

Epoch 73: val_loss did not improve from 0.28756
11/11 - 2s - loss: 0.2941 - mse: 0.2941 - mae: 0.4394 - val_loss: 0.2886 - val_mse: 0.2886 - val_mae: 0.4346 - 2s/epoch - 173ms/step
Epoch 74/10000

Epoch 74: val_loss improved from 0.28756 to 0.28730, saving model to ./results/NN_rms_regr/RNN/recursion_47/ckpt_1
11/11 - 3s - loss: 0.2933 - mse: 0.2933 - mae: 0.4390 - val_loss: 0.2873 - val_mse: 0.2873 - val_mae: 0.4338 - 3s/epoch - 256ms/step
Epoch 75/10000

Epoch 75: val_loss did not improve from 0.28730
11/11 - 2s - loss: 0.2934 - mse: 0.2934 - mae: 0.4390 - val_loss: 0.2880 - val_mse: 0.2880 - val_mae: 0.4341 - 2s/epoch - 174ms/step
Epoch 76/10000

Epoch 76: val_loss did not improve from 0.28730
11/11 - 2s - loss: 0.2931 - mse: 0.2931 - mae: 0.4388 - val_loss: 0.2886 - val_mse: 0.2886 - val_mae: 0.4348 - 2s/epoch - 174ms/step
Epoch 77/10000

Epoch 77: val_loss improved from 0.28730 to 0.28729, saving model to ./results/NN_rms_regr/RNN/recursion_47/ckpt_1
11/11 - 3s - loss: 0.2946 - mse: 0.2946 - mae: 0.4394 - val_loss: 0.2873 - val_mse: 0.2873 - val_mae: 0.4338 - 3s/epoch - 255ms/step
Epoch 78/10000

Epoch 78: val_loss did not improve from 0.28729
11/11 - 2s - loss: 0.2942 - mse: 0.2942 - mae: 0.4396 - val_loss: 0.2876 - val_mse: 0.2876 - val_mae: 0.4338 - 2s/epoch - 175ms/step
Epoch 79/10000

Epoch 79: val_loss did not improve from 0.28729
11/11 - 2s - loss: 0.2930 - mse: 0.2930 - mae: 0.4385 - val_loss: 0.2876 - val_mse: 0.2876 - val_mae: 0.4340 - 2s/epoch - 174ms/step
Epoch 80/10000

Epoch 80: val_loss did not improve from 0.28729
11/11 - 2s - loss: 0.2933 - mse: 0.2933 - mae: 0.4387 - val_loss: 0.2874 - val_mse: 0.2874 - val_mae: 0.4338 - 2s/epoch - 174ms/step
Epoch 81/10000

Epoch 81: val_loss improved from 0.28729 to 0.28704, saving model to ./results/NN_rms_regr/RNN/recursion_47/ckpt_1
11/11 - 3s - loss: 0.2930 - mse: 0.2930 - mae: 0.4384 - val_loss: 0.2870 - val_mse: 0.2870 - val_mae: 0.4334 - 3s/epoch - 255ms/step
Epoch 82/10000

Epoch 82: val_loss did not improve from 0.28704
11/11 - 2s - loss: 0.2928 - mse: 0.2928 - mae: 0.4382 - val_loss: 0.2871 - val_mse: 0.2871 - val_mae: 0.4334 - 2s/epoch - 173ms/step
Epoch 83/10000
